{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3227e9fe-a927-4b19-b903-a51731212438",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run IsWrls_eMMB_WmmsePowerControl.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, Sigmoid, BatchNorm1d as BN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19786971-54cf-4dd0-805c-57e7d0b119ed",
   "metadata": {},
   "source": [
    "1. **Class Definition:**\n",
    "   - Define a class named `IGConv` which inherits from `MessagePassing`.\n",
    "\n",
    "2. **Constructor (`__init__` method):**\n",
    "   - Initialize the message passing aggregation method to 'max' and other keyword arguments.\n",
    "   - Define two multi-layer perceptrons (MLPs) named `mlp1` and `mlp2`.\n",
    "   - Call the constructor of the superclass (`MessagePassing`).\n",
    "\n",
    "3. **Parameter Initialization (`reset_parameters` method):**\n",
    "   - Reset the parameters of `mlp1` and `mlp2`.\n",
    "\n",
    "4. **Update Function (`update` method):**\n",
    "   - Concatenate the input features (`x`) and aggregated neighbor features (`aggr_out`).\n",
    "   - Pass the concatenated features through `mlp2` to obtain combined features.\n",
    "   - Concatenate the original features (`x[:,:2]`) with the combined features (`comb`) along the feature dimension.\n",
    "\n",
    "5. **Forward Pass (`forward` method):**\n",
    "   - Ensure that input features (`x`) and edge attributes (`edge_attr`) have the correct dimensions.\n",
    "   - Use the `propagate` method to perform message passing based on the given edge indices (`edge_index`), input features (`x`), and edge attributes (`edge_attr`).\n",
    "\n",
    "6. **Message Function (`message` method):**\n",
    "   - Concatenate the features of the receiving node (`x_j`) with the edge attributes (`edge_attr`) and pass them through `mlp1` to obtain aggregated messages.\n",
    "\n",
    "7. **Representation Function (`__repr__` method):**\n",
    "   - Return a string representation of the class name (`IGConv`) along with the parameters `mlp1` and `mlp2`.\n",
    "\n",
    "8. **Mathematical Formulas:**\n",
    "   - Let $x_i$ and $x_j$ be the features of the sending and receiving nodes, respectively.\n",
    "   - Let $\\text{edge\\_attr}$ be the edge attributes.\n",
    "   - $\\text{mlp1}$ and $\\text{mlp2}$ represent the multi-layer perceptron layers.\n",
    "   - $\\text{agg}$ represents the aggregated messages obtained after passing through $\\text{mlp1}$.\n",
    "   - $\\text{comb}$ represents the combined features obtained after passing through $\\text{mlp2}$.\n",
    "   - $\\text{aggr\\_out}$ represents the aggregated neighbor features after message passing.\n",
    "\n",
    "9. **Model Overview:**\n",
    "   - The class implements a graph convolutional layer using message passing.\n",
    "   - It utilizes two multi-layer perceptrons (`mlp1` and `mlp2`) to aggregate messages and update node features.\n",
    "   - The `update` method combines the original node features with the aggregated features to produce updated node representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "745059d7-dbac-4b3f-94b0-009b4d17bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGConv(MessagePassing):\n",
    "    def __init__(self, mlp1, mlp2, **kwargs):\n",
    "        super(IGConv, self).__init__(aggr='max', **kwargs)\n",
    "\n",
    "        self.mlp1 = mlp1\n",
    "        self.mlp2 = mlp2\n",
    "        #self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.mlp1)\n",
    "        reset(self.mlp2)\n",
    "        \n",
    "    def update(self, aggr_out, x):\n",
    "        tmp = torch.cat([x, aggr_out], dim=1)\n",
    "        comb = self.mlp2(tmp)\n",
    "        return torch.cat([x[:,:2], comb],dim=1)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "        edge_attr = edge_attr.unsqueeze(-1) if edge_attr.dim() == 1 else edge_attr\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        tmp = torch.cat([x_j, edge_attr], dim=1)\n",
    "        agg = self.mlp1(tmp)\n",
    "        return agg\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(nn={})'.format(self.__class__.__name__, self.mlp1,self.mlp2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f34ab7-e0fa-4b1f-9bfa-cfbb48835e40",
   "metadata": {},
   "source": [
    "1. **Input Parameters:**\n",
    "   - `channels`: List of integers representing the number of input, hidden, and output units in each layer of the MLP.\n",
    "   - `batch_norm`: Boolean indicating whether batch normalization should be applied after each linear layer.\n",
    "\n",
    "2. **Multi-Layer Perceptron (MLP) Construction:**\n",
    "   - Define a function named `MLP` that takes the list of `channels` as input and returns a sequential composition of linear layers followed by activation functions (ReLU).\n",
    "   - For each layer $i$ in the range from 1 to the length of `channels`, excluding the first element:\n",
    "     - Create a linear transformation (`Lin`) from the previous layer's output size (`channels[i - 1]`) to the current layer's output size (`channels[i]`).\n",
    "     - Apply the rectified linear unit (ReLU) activation function to introduce non-linearity.\n",
    "     - Optionally, apply batch normalization (BN) after each linear layer if `batch_norm` is set to `True`.\n",
    "\n",
    "3. **Mathematical Formulas:**\n",
    "   - Let `channels = [n_0, n_1, ..., n_L]`, where $n_0$ is the input size, $n_L$ is the output size, and $n_i$ represents the number of units in hidden layer $i$.\n",
    "   - Define `Lin(n_{i-1}, n_i)` as a linear transformation from layer $i-1$ to layer $i$.\n",
    "   - Apply the rectified linear unit (ReLU) activation function: $\\text{ReLU}(x) = \\max(0, x)$.\n",
    "   - Optionally, apply batch normalization (BN) after each linear layer to normalize the activations.\n",
    "\n",
    "4. **Model Overview:**\n",
    "   - The function constructs a multi-layer perceptron (MLP) neural network architecture with fully connected layers and ReLU activation functions.\n",
    "   - The number of layers and units in each layer are specified by the `channels` parameter.\n",
    "   - Batch normalization can be optionally included after each linear layer if `batch_norm` is set to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed43bcee-955a-4f0f-9a0c-a821cf318fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(channels, batch_norm=True):\n",
    "    return Seq(*[\n",
    "        Seq(Lin(channels[i - 1], channels[i], bias = True), ReLU())#, BN(channels[i]))\n",
    "        for i in range(1, len(channels))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf051ca-c2a4-4ce7-b867-867a0cf862f3",
   "metadata": {},
   "source": [
    "1. **Class Definition:**\n",
    "   - Define a class named `IGCNet` which inherits from `torch.nn.Module`.\n",
    "\n",
    "2. **Constructor (`__init__` method):**\n",
    "   - Initialize the module using `super(IGCNet, self).__init__()`.\n",
    "   - Define layers:\n",
    "     - `mlp1`: Multi-layer perceptron (MLP) with input size 5, hidden layer sizes [16, 32].\n",
    "     - `mlp2`: Another MLP with input size 35 and a single output neuron.\n",
    "     - `conv`: Instance of `IGConv` with `mlp1` and `mlp2` as parameters.\n",
    "\n",
    "3. **Forward Pass (`forward` method):**\n",
    "   - Accepts a `data` object containing features, edge attributes, and edge indices.\n",
    "   - Extract features (`x0`), edge attributes (`edge_attr`), and edge indices (`edge_index`) from the `data` object.\n",
    "   - Perform convolution operations using the `conv` layer:\n",
    "     - Pass the input features, edge attributes, and edge indices through the convolutional layer (`self.conv`).\n",
    "     - Repeat the convolution operation twice (`x1` and `x2`) for feature refinement.\n",
    "   - Return the final output (`out`) obtained after multiple convolution operations.\n",
    "\n",
    "4. **Mathematical Formulas:**\n",
    "   - Let $x_0$ be the input feature matrix.\n",
    "   - Let $\\text{edge\\_attr}$ be the edge attribute matrix.\n",
    "   - Let $\\text{edge\\_index}$ be the edge index tensor.\n",
    "   - $\\text{mlp1}$ and $\\text{mlp2}$ represent the multi-layer perceptron layers.\n",
    "   - $\\text{conv}$ represents the graph convolutional layer (`IGConv`).\n",
    "   - $x_1$ and $x_2$ represent the output features after the first and second convolutional layers, respectively.\n",
    "   - $\\text{out}$ represents the final output after multiple convolution operations.\n",
    "\n",
    "5. **Model Overview:**\n",
    "   - The model utilizes a graph convolutional neural network (GCN) architecture to process graph-structured data.\n",
    "   - It applies multiple graph convolution operations to refine the node features and capture complex relationships within the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4afb4f20-92bd-45ad-a0ed-db6aea5c32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGCNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IGCNet, self).__init__()\n",
    "\n",
    "        self.mlp1 = MLP([5, 16, 32])\n",
    "        self.mlp2 = MLP([35, 16])\n",
    "        self.mlp2 = Seq(*[self.mlp2,Seq(Lin(16, 1, bias = True), Sigmoid())])\n",
    "        self.conv = IGConv(self.mlp1,self.mlp2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x0, edge_attr, edge_index = data.x, data.edge_attr, data.edge_index\n",
    "        x1 = self.conv(x = x0, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        x2 = self.conv(x = x1, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        #x3 = self.conv(x = x2, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        #x4 = self.conv(x = x3, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        out = self.conv(x = x2, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1cd24-e01d-42d0-a5b2-93e9767c5be4",
   "metadata": {},
   "source": [
    "1. **Input Parameter:**\n",
    "   - $n$: Number of nodes in the graph.\n",
    "\n",
    "2. **Generating Complete Graph:**\n",
    "   - Initialize an empty list $\\text{adj}$ to store the adjacency list representing the graph.\n",
    "   - For each node $i$ from $0$ to $n-1$:\n",
    "     - For each node $j$ from $0$ to $n-1$:\n",
    "       - If $i \\neq j$, indicating that the edge is not a self-loop:\n",
    "         - Add the edge $(i, j)$ to the adjacency list $\\text{adj}$.\n",
    "\n",
    "3. **Output:**\n",
    "   - $\\text{adj}$: Adjacency list representing the complete graph, where each element is a pair of nodes representing an edge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6368bd1a-9d69-44ab-83b9-a16ec98405ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cg(n):\n",
    "    adj = []\n",
    "    for i in range(0,n):\n",
    "        for j in range(0,n):\n",
    "            if(not(i==j)):\n",
    "                adj.append([i,j])\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792601e-b732-43f8-b354-dc489acda65e",
   "metadata": {},
   "source": [
    "1. **Input Parameters:**\n",
    "   - $H$: Channel gains matrix of shape $N \\times K \\times 1$, where $N$ is the number of samples, and $K$ is the number of users.\n",
    "   - $p$: Transmit powers matrix of shape $N \\times K \\times 1 \\times N$.\n",
    "   - $\\alpha$: User-specific weights matrix of shape $N \\times K$.\n",
    "   - $\\text{var\\_noise}$: Variance of the noise.\n",
    "\n",
    "2. **Expanding Dimensions:**\n",
    "   - Expand the dimensions of $H$ to include an additional singleton dimension at the end: $H = \\text{np.expand\\_dims}(H, axis=-1)$.\n",
    "\n",
    "3. **Calculating Received Power:**\n",
    "   - $\\text{rx\\_power} = H \\cdot p$ (element-wise multiplication)\n",
    "   - $\\text{rx\\_power} = \\sum(\\text{rx\\_power}, axis=-1)$ (sum along the last dimension)\n",
    "   - $\\text{rx\\_power} = |\\text{rx\\_power}|^2$ (square of absolute values)\n",
    "\n",
    "4. **Calculating Valid and Interference Powers:**\n",
    "   - $\\text{mask} = \\text{np.eye}(K)$ (identity matrix of size $K \\times K$)\n",
    "   - $\\text{valid\\_rx\\_power} = \\sum(\\text{rx\\_power} \\cdot \\text{mask}, axis=1)$ (sum of diagonal elements)\n",
    "   - $\\text{interference} = \\sum(\\text{rx\\_power} \\cdot (1 - \\text{mask}), axis=1) + \\text{var\\_noise}$ (sum of off-diagonal elements)\n",
    "\n",
    "5. **Calculating Rates:**\n",
    "   - $\\text{rate} = \\log\\left(1 + \\frac{\\text{valid\\_rx\\_power}}{\\text{interference}}\\right)$\n",
    "\n",
    "6. **Weighted Rates:**\n",
    "   - $\\text{w\\_rate} = \\alpha \\cdot \\text{rate}$\n",
    "\n",
    "7. **Calculating Sum Rate:**\n",
    "   - $\\text{sum\\_rate} = \\frac{1}{N} \\sum(\\sum(\\text{w\\_rate}, axis=1))$ (mean of sum of weighted rates across all samples)\n",
    "\n",
    "8. **Output:**\n",
    "   - $\\text{sum\\_rate}$: Average sum rate over all samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88ce37da-524a-4e2d-ac77-4fb48c52848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_sum_rate(H,p,alpha,var_noise):\n",
    "    H = np.expand_dims(H,axis=-1)\n",
    "    K = H.shape[1]\n",
    "    N = H.shape[-1]\n",
    "    p = p.reshape((-1,K,1,N))\n",
    "    rx_power = np.multiply(H, p)\n",
    "    rx_power = np.sum(rx_power,axis=-1)\n",
    "    rx_power = np.square(abs(rx_power))\n",
    "    mask = np.eye(K)\n",
    "    valid_rx_power = np.sum(np.multiply(rx_power, mask), axis=1)\n",
    "    interference = np.sum(np.multiply(rx_power, 1 - mask), axis=1) + var_noise\n",
    "    rate = np.log(1 + np.divide(valid_rx_power, interference))\n",
    "    w_rate = np.multiply(alpha,rate)\n",
    "    sum_rate = np.mean(np.sum(w_rate, axis=1))\n",
    "    return sum_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e58615-2049-4d65-9c20-6186f8bfcd96",
   "metadata": {},
   "source": [
    "1. **Input Parameters:**\n",
    "   - $X$: Input data matrix of shape $n \\times K \\times K$, where $n$ is the number of samples, and $K$ is the number of users.\n",
    "   - $AAA$: User-specific scaling factors matrix of shape $n \\times K$.\n",
    "   - $label$: Label vector indicating the selected users for each sample.\n",
    "\n",
    "2. **Initialization:**\n",
    "   - $n = X.shape[0]$ (number of samples)\n",
    "   - $\\text{thd} = \\frac{\\sum \\text{label}}{n}$ (threshold calculated based on the label vector)\n",
    "\n",
    "3. **Greedy User Selection:**\n",
    "   - Initialize $Y$ as a zero matrix of shape $n \\times K$.\n",
    "   - For each sample $ii$ from $1$ to $n$:\n",
    "     - Extract the user-specific scaling factors $\\alpha$ for the $ii$-th sample.\n",
    "     - Calculate $H_{\\text{diag}} = \\alpha \\cdot (\\text{diagonal of } X[ii,:,:])^2$.\n",
    "     - Sort the indices of $H_{\\text{diag}}$ in descending order and store them in $xx$.\n",
    "     - Select the top $\\text{thd}$ indices from $xx$ and set the corresponding elements in $Y[ii,:]$ to 1.\n",
    "\n",
    "4. **Output:**\n",
    "   - $Y$: Matrix indicating the selected users for each sample, where each row represents a sample, and each column represents a user. A value of 1 indicates that the corresponding user is selected for the respective sample, while 0 indicates non-selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "869c1213-8a8d-418d-8a8c-9da8dd7a3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_greedy(X,AAA,label):\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    thd = int(np.sum(label)/n)\n",
    "    Y = np.zeros((n,K))\n",
    "    for ii in range(n):\n",
    "        alpha = AAA[ii,:]\n",
    "        H_diag = alpha * np.square(np.diag(X[ii,:,:]))\n",
    "        xx = np.argsort(H_diag)[::-1]\n",
    "        for jj in range(thd):\n",
    "            Y[ii,xx[jj]] = 1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649889c-03e2-4075-a7f0-377171f5e6f4",
   "metadata": {},
   "source": [
    "1. **Input Parameters:**\n",
    "   - $H$: Channel gains matrix of shape $K \\times K$, where $K$ is the number of users.\n",
    "   - $A$: User-specific scaling factors matrix of shape $K \\times 1$.\n",
    "   - $\\text{adj}$: Adjacency list representing the graph structure.\n",
    "\n",
    "2. **Building Graph Representation:**\n",
    "   - $n = H.shape[0]$ (number of nodes in the graph)\n",
    "   - $x_1 = \\text{np.expand\\_dims}(\\text{diag}(H), axis=1)$ (expand dimensions to represent diagonal elements of $H$)\n",
    "   - $x_2 = \\text{np.expand\\_dims}(A, axis=1)$ (expand dimensions of $A$)\n",
    "   - $x_3 = \\text{np.ones}((K,1))$ (create a column vector of ones)\n",
    "   - Concatenate $x_1$, $x_2$, and $x_3$ to form feature vectors $x$\n",
    "   - Initialize an empty list $\\text{edge\\_attr}$ to store edge attributes\n",
    "   - For each edge $e$ in the adjacency list $\\text{adj}$:\n",
    "     - Compute the edge attributes based on the corresponding channel gains in $H$ and add them to $\\text{edge\\_attr}$\n",
    "   - Convert feature vectors $x$, edge indices, edge attributes, node labels $y$, and node positions $\\text{pos}$ into PyTorch tensors\n",
    "   - Create a `Data` object using the tensors to represent the graph data.\n",
    "\n",
    "3. **Output:**\n",
    "   - `data`: PyTorch `Data` object representing the graph, containing features, edge indices, edge attributes, node labels, and node positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5f3a832-9d1f-4dad-be50-24711d2803a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(H,A,adj):\n",
    "    n = H.shape[0]\n",
    "    x1 = np.expand_dims(np.diag(H),axis=1)\n",
    "    x2 = np.expand_dims(A,axis=1)\n",
    "    x3 = np.ones((K,1))\n",
    "    edge_attr = []\n",
    "    \n",
    "    x = np.concatenate((x1,x2,x3),axis=1)\n",
    "    for e in adj:\n",
    "        edge_attr.append([H[e[0],e[1]],H[e[1],e[0]]])\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    edge_index = torch.tensor(adj, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    y = torch.tensor(np.expand_dims(H,axis=0), dtype=torch.float)\n",
    "    pos = torch.tensor(np.expand_dims(A,axis=0), dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(),edge_attr = edge_attr, y = y, pos = pos)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776b0ee-376b-4aa7-8326-9110c69199b4",
   "metadata": {},
   "source": [
    "1. **Input Parameters:**\n",
    "   - $HH$: Channel gains matrices of shape $n \\times K \\times K$, where $n$ is the number of samples, and $K$ is the number of users.\n",
    "   - $AA$: User-specific scaling factors matrices of shape $n \\times K$, where $n$ is the number of samples, and $K$ is the number of users.\n",
    "\n",
    "2. **Processing Data:**\n",
    "   - Initialize an empty list $\\text{data\\_list}$ to store processed data for each sample.\n",
    "   - Obtain the channel gain graph ($cg$) using the function $\\text{get\\_cg}(K)$, where $K$ is the number of users.\n",
    "   - For each sample $i$ from $1$ to $n$:\n",
    "     - Build the graph representation of the channel gains and user-specific scaling factors using the function $\\text{build\\_graph}$.\n",
    "     - Append the processed data (graph representation) to $\\text{data\\_list}$.\n",
    "\n",
    "3. **Output:**\n",
    "   - $\\text{data\\_list}$: List containing the processed data for each sample, where each element represents the graph representation of the channel gains and user-specific scaling factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c46d808-458f-4683-9ee0-2b73cdf9cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_data(HH,AA):\n",
    "    n = HH.shape[0]\n",
    "    data_list = []\n",
    "    cg = get_cg(K)\n",
    "    for i in range(n):\n",
    "        data = build_graph(HH[i],AA[i],cg)\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e555153-9d80-4434-ba7a-b2e72cf25e59",
   "metadata": {},
   "source": [
    "# Function: generate_wGaussian\n",
    "\n",
    "This function generates data for a Gaussian channel with interference.\n",
    "\n",
    "### Inputs:\n",
    "- $K$: Number of users.\n",
    "- $num\\_H$: Number of channel realizations.\n",
    "- $var\\_noise$: Variance of the noise.\n",
    "- $P_{min}$: Minimum transmit power.\n",
    "- $seed$: Random seed for reproducibility.\n",
    "\n",
    "### Steps:\n",
    "1. **Set Seed:**\n",
    "   - Set the random seed for reproducibility using the input parameter.\n",
    "\n",
    "2. **Initialize Parameters:**\n",
    "   - Set the maximum transmit power $P_{max} = 1$.\n",
    "   - Initialize the transmit power matrix $P_{ini}$ with all elements set to $P_{max}$.\n",
    "   - Generate random channel coefficients $\\alpha$ for each channel realization.\n",
    "   - Generate a matrix $X$ filled with zeros.\n",
    "   - Initialize matrices $Y$ and $Y_2$ with zeros.\n",
    "\n",
    "3. **Generate Channel Coefficients:**\n",
    "   - Generate complex channel coefficients $CH$ with real and imaginary parts sampled from a standard normal distribution.\n",
    "   - Compute the absolute values of the channel coefficients to obtain the channel gains $H$.\n",
    "\n",
    "4. **Compute Channel Outputs:**\n",
    "   - Compute the channel outputs $Y$ and $Y_2$ using the WMMSE algorithm (`batch_WMMSE2`) with the generated channel gains $H$, transmit powers $P_{ini$, and noise variance $var\\_noise$.\n",
    "   - $Y$ represents the WMMSE output with randomly generated weights $\\alpha$.\n",
    "   - $Y_2$ represents the WMMSE output with fixed weights (all ones).\n",
    "\n",
    "### Outputs:\n",
    "- $H$: Channel gains matrix.\n",
    "- $Y$: WMMSE output with randomly generated weights.\n",
    "- $alpha$: Randomly generated weights.\n",
    "- $Y_2$: WMMSE output with fixed weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4482aba-2a9d-4dac-ada3-984b7785015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wGaussian(K, num_H, var_noise=1, Pmin=0, seed=2017):\n",
    "    print('Generate Data ... (seed = %d)' % seed)\n",
    "    np.random.seed(seed)\n",
    "    Pmax = 1\n",
    "    Pini = Pmax*np.ones((num_H,K,1) )\n",
    "    #alpha = np.random.rand(num_H,K)\n",
    "    alpha = np.random.rand(num_H,K)\n",
    "    #alpha = np.ones((num_H,K))\n",
    "    fake_a = np.ones((num_H,K))\n",
    "    #var_noise = 1\n",
    "    X=np.zeros((K**2,num_H))\n",
    "    Y=np.zeros((K,num_H))\n",
    "    total_time = 0.0\n",
    "    CH = 1/np.sqrt(2)*(np.random.randn(num_H,K,K)+1j*np.random.randn(num_H,K,K))\n",
    "    H=abs(CH)\n",
    "    Y = batch_WMMSE2(Pini,alpha,H,Pmax,var_noise)\n",
    "    Y2 = batch_WMMSE2(Pini,fake_a,H,Pmax,var_noise)\n",
    "    return H, Y, alpha, Y2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d58932a-bf3b-4d5c-9e7b-9fdd11d26596",
   "metadata": {},
   "source": [
    "1. **Function Definition:**\n",
    "   - Define a function named `sr_loss` responsible for computing the loss between the model output and the ground truth.\n",
    "\n",
    "2. **Input Parameters:**\n",
    "   - $data$: Input data containing the ground truth information.\n",
    "   - $out$: Model output containing the predicted power allocations.\n",
    "   - $K$: Number of users in the system.\n",
    "\n",
    "3. **Power Allocation Extraction:**\n",
    "   - Extract the power allocations from the model output $out$.\n",
    "   - Reshape the power allocations to match the shape of the input data.\n",
    "\n",
    "4. **Calculate Received Power:**\n",
    "   - Compute the received power at each receiver by multiplying the squared absolute channel gains $abs\\_H\\_2$ with the corresponding power allocations.\n",
    "\n",
    "5. **Masking Valid Receiver Power:**\n",
    "   - Apply a masking matrix to extract the valid receiver power, considering only the diagonal elements of the received power matrix.\n",
    "\n",
    "6. **Calculate Interference:**\n",
    "   - Compute the interference by summing the received power over all interfering users and adding the noise variance $var$.\n",
    "\n",
    "7. **Compute Rate:**\n",
    "   - Calculate the achievable rate for each user using the Shannon capacity formula: $rate = \\log/_2(1 + \\frac{valid/_rx/_power}{interference})$.\n",
    "\n",
    "8. **Weighted Rate Calculation:**\n",
    "   - Multiply the achievable rate for each user by the corresponding user position $data.pos$, effectively weighting the rate by the user's position.\n",
    "\n",
    "9. **Compute Sum Rate:**\n",
    "   - Calculate the sum rate by averaging the weighted rates across all users.\n",
    "\n",
    "10. **Loss Calculation:**\n",
    "    - Negate the sum rate to obtain the loss value.\n",
    "    - Since the goal is to maximize the sum rate, negating it turns the optimization problem into a minimization problem.\n",
    "\n",
    "11. **Mathematical Formulas:**\n",
    "    - Let $ \\mathcal{D} = \\{ (\\mathbf{X}_{i}, \\mathbf{Y}_{i}) \\}_{i=1}^{\\text{num\\_test}} $ represent the input data, where $ \\mathbf{X}_i $ is the input data and $ \\mathbf{Y}_i $ is the corresponding ground truth.\n",
    "    - Let $ out $ represent the model output containing the predicted power allocations.\n",
    "    - Let $ K $ be the number of users in the system.\n",
    "    - Let $ power $ represent the extracted power allocations from the model output.\n",
    "    - Let $ abs/_H $ represent the absolute channel gains.\n",
    "    - Let $ rx/_power $ represent the received power at each receiver.\n",
    "    - Let $ mask $ represent the masking matrix to extract valid receiver power.\n",
    "    - Let $ valid/_rx/_power $ represent the valid receiver power after applying the masking matrix.\n",
    "    - Let $ interference $ represent the interference experienced by each receiver.\n",
    "    - Let $ rate $ represent the achievable rate for each user.\n",
    "    - Let $ w/_rate $ represent the weighted rate for each user based on their position.\n",
    "    - Let $ sum/_rate $ represent the sum rate computed by averaging the weighted rates across all users.\n",
    "    - The loss function is computed as the negative of the sum rate to be minimized during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10747267-07f3-4880-b7b2-131cb563a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sr_loss(data, out, K):\n",
    "    power = out[:,2]\n",
    "    power = torch.reshape(power, (-1, K, 1)) \n",
    "    abs_H = data.y\n",
    "    abs_H_2 = torch.pow(abs_H, 2)  \n",
    "    rx_power = torch.mul(abs_H_2, power)\n",
    "    mask = torch.eye(K)\n",
    "    mask = mask.to(device)\n",
    "    valid_rx_power = torch.sum(torch.mul(rx_power, mask), 1)\n",
    "    interference = torch.sum(torch.mul(rx_power, 1 - mask), 1) + var\n",
    "    rate = torch.log(1 + torch.div(valid_rx_power, interference))\n",
    "    w_rate = torch.mul(data.pos,rate)\n",
    "    sum_rate = torch.mean(torch.sum(w_rate, 1))\n",
    "    loss = torch.neg(sum_rate)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ff664-c34b-4550-b60c-8e3aa5c597ea",
   "metadata": {},
   "source": [
    "1. **Function Definition:**\n",
    "   - Define a function named `train` responsible for training the model.\n",
    "\n",
    "2. **Training Loop:**\n",
    "   - Set the model to training mode (`model.train()`).\n",
    "   - Initialize the total loss variable (`total_loss`) to 0.\n",
    "   - Iterate over the training data batches using the `train_loader`.\n",
    "   - For each batch of data:\n",
    "     - Move the data to the appropriate device (CPU or GPU).\n",
    "     - Zero out the gradients of the optimizer (`optimizer.zero_grad()`).\n",
    "     - Pass the data through the model (`model(data)`).\n",
    "     - Calculate the loss between the model output and the ground truth using the `sr_loss` function.\n",
    "     - Backpropagate the gradients (`loss.backward()`).\n",
    "     - Update the total loss with the current batch loss multiplied by the number of graphs in the batch.\n",
    "     - Update the model parameters using the optimizer (`optimizer.step()`).\n",
    "\n",
    "3. **Loss Calculation:**\n",
    "   - The loss function (`sr_loss`) computes the loss between the model output and the ground truth.\n",
    "   - The total loss is the average loss across all batches, normalized by the total number of training samples (`num_H`).\n",
    "\n",
    "4. **Mathematical Formulas:**\n",
    "   - Let $\\mathcal{D} = \\{(\\mathbf{X}_i, \\mathbf{Y}_i)\\}_{i=1}^{\\text{num\\_H}}$ represent the training dataset, where $\\mathbf{X}_i$ is the input data and $\\mathbf{Y}_i$ is the corresponding ground truth.\n",
    "   - Let $\\text{train\\_loader}$ be the data loader that provides batches of data from $\\mathcal{D}$.\n",
    "   - Let $\\text{model}$ represent the neural network model being trained.\n",
    "   - Let $\\text{optimizer}$ be the optimization algorithm used to update the model parameters.\n",
    "   - Let $\\text{loss}(\\mathbf{Y}_{\\text{true}}, \\mathbf{Y}_{\\text{pred}})$ be the loss function used to measure the discrepancy between the true and predicted values.\n",
    "   - The total loss $\\mathcal{L}$ is calculated as the average loss across all training samples:\n",
    "     $$ \\mathcal{L} = \\frac{1}{\\text{num\\_H}} \\sum_{i=1}^{\\text{num\\_H}} \\text{loss}(\\mathbf{Y}_i, \\text{model}(\\mathbf{X}_i)) $$\n",
    "\n",
    "5. **Model Update:**\n",
    "   - The optimizer updates the model parameters using the gradients obtained from backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4996568a-94f4-4909-add8-bb38514f76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = sr_loss(data,out,K)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / num_H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ba081-535d-4ada-ad7c-3621deafd746",
   "metadata": {},
   "source": [
    "1. **Function Definition:**\n",
    "   - Define a function named `test` responsible for evaluating the model's performance on the test dataset.\n",
    "\n",
    "2. **Evaluation Loop:**\n",
    "   - Set the model to evaluation mode (`model.eval()`).\n",
    "   - Initialize the total loss variable (`total_loss`) to 0.\n",
    "   - Iterate over the test data batches using the `test_loader`.\n",
    "   - For each batch of data:\n",
    "     - Move the data to the appropriate device (CPU or GPU).\n",
    "     - Disable gradient calculation to save memory and computation (`torch.no_grad()`).\n",
    "     - Pass the data through the model (`model(data)`).\n",
    "     - Calculate the loss between the model output and the ground truth using the `sr_loss` function.\n",
    "     - Update the total loss with the current batch loss multiplied by the number of graphs in the batch.\n",
    "\n",
    "3. **Loss Calculation:**\n",
    "   - The loss function (`sr_loss`) computes the loss between the model output and the ground truth.\n",
    "   - The total loss is the average loss across all batches, normalized by the total number of testing samples (`num_test`).\n",
    "\n",
    "4. **Mathematical Formulas:**\n",
    "   - Let $\\mathcal{D} = \\{(\\mathbf{X}_i, \\mathbf{Y}_i)\\}_{i=1}^{\\text{num\\_test}}$ represent the test dataset, where $\\mathbf{X}_i$ is the input data and $\\mathbf{Y}_i$ is the corresponding ground truth.\n",
    "   - Let $\\text{test\\_loader}$ be the data loader that provides batches of data from $\\mathcal{D}$.\n",
    "   - Let $\\text{model}$ represent the neural network model being evaluated.\n",
    "   - The total loss $\\mathcal{L}$ is calculated as the average loss across all testing samples:\n",
    "     $$ \\mathcal{L} = \\frac{1}{\\text{num\\_test}} \\sum_{i=1}^{\\text{num\\_test}} \\text{loss}(\\mathbf{Y}_i, \\text{model}(\\mathbf{X}_i)) $$\n",
    "\n",
    "5. **Model Evaluation:**\n",
    "   - The model's performance is evaluated by computing the loss on the test dataset, providing insight into its generalization ability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20bdd9fb-658b-49ca-9130-503207469bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            loss = sr_loss(data,out,K)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / num_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6aa66d-3cdd-4401-9912-cd6e5646db5f",
   "metadata": {},
   "source": [
    "# Gaussian IC Case: $K=10$, Total Samples: $10000$, Total Iterations: $50$\n",
    "\n",
    "- Set the parameters for the Gaussian Interference Channel (IC) scenario:\n",
    "  - Number of users: $K = 10$\n",
    "  - Total training samples: $10000$\n",
    "  - Total testing samples: $2000$\n",
    "  - Number of training epochs: $50$\n",
    "  - Random seed for training set: $0$\n",
    "  - Random seed for test set: $7$\n",
    "\n",
    "- Calculate the variance for Gaussian noise based on a given variance in decibels ($\\text{var}\\_\\text{db} = 10$) using the formula:\n",
    "  $ \\text{var} = \\dfrac{1}{10^{\\dfrac{\\text{var}\\_\\text{db}}{10}}} $\n",
    "\n",
    "- Generate training and testing data using the `generate_wGaussian` function with the specified parameters.\n",
    "\n",
    "- Perform the greedy algorithm (`simple_greedy`) on the testing data to obtain the baseline solution (`bl_Y`). Compute the sum rate using the `np_sum_rate` function.\n",
    "\n",
    "- Compute the sum rate for the testing data using the Weighted Minimum Mean Squared Error (WMMSE) algorithm:\n",
    "  - For the weighted WMMSE (`wmmse`) algorithm.\n",
    "  - For the unweighted WMMSE (`wmmse unweighted`) algorithm.\n",
    "\n",
    "- Preprocess the training and testing data using the `proc_data` function.\n",
    "\n",
    "- Initialize the model (`IGCNet`) and move it to the appropriate device (GPU if available, otherwise CPU).\n",
    "\n",
    "- Set up the optimizer (Adam) with a learning rate of $0.001$.\n",
    "\n",
    "- Set up a scheduler to adjust the learning rate based on the number of epochs.\n",
    "\n",
    "- Create data loaders for training and testing data.\n",
    "\n",
    "- Perform training for a total of $200$ epochs:\n",
    "  - Compute the training loss (`loss1`) using the `train` function.\n",
    "  - Compute the validation loss (`loss2`) using the `test` function every $8$ epochs.\n",
    "  - Adjust the learning rate using the scheduler.\n",
    "\n",
    "- Output the epoch number, training loss, and validation loss every $8$ epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57113931-2d24-42a7-9710-f41c9f575402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian IC Case: K=10, Total Samples: 10000, Total Iterations: 50\n",
      "\n",
      "Generate Data ... (seed = 0)\n",
      "Generate Data ... (seed = 7)\n",
      "greedy: 2.2770127871958885\n",
      "wmmse: 2.504327512048192\n",
      "wmmse unweighted: 1.8971656708039755\n",
      "False\n",
      "Epoch 008, Train Loss: -2.4998, Val Loss: -2.4998\n",
      "Epoch 016, Train Loss: -2.5193, Val Loss: -2.5233\n",
      "Epoch 024, Train Loss: -2.5191, Val Loss: -2.5191\n",
      "Epoch 032, Train Loss: -2.5306, Val Loss: -2.5334\n",
      "Epoch 040, Train Loss: -2.5309, Val Loss: -2.5428\n",
      "Epoch 048, Train Loss: -2.5350, Val Loss: -2.5447\n",
      "Epoch 056, Train Loss: -2.5442, Val Loss: -2.5430\n",
      "Epoch 064, Train Loss: -2.5444, Val Loss: -2.5491\n",
      "Epoch 072, Train Loss: -2.5461, Val Loss: -2.5516\n",
      "Epoch 080, Train Loss: -2.5478, Val Loss: -2.5399\n",
      "Epoch 088, Train Loss: -2.5355, Val Loss: -2.4874\n",
      "Epoch 096, Train Loss: -2.5554, Val Loss: -2.5572\n",
      "Epoch 104, Train Loss: -2.5574, Val Loss: -2.5598\n",
      "Epoch 112, Train Loss: -2.5568, Val Loss: -2.5628\n",
      "Epoch 120, Train Loss: -2.5570, Val Loss: -2.5481\n",
      "Epoch 128, Train Loss: -2.5601, Val Loss: -2.5587\n",
      "Epoch 136, Train Loss: -2.5583, Val Loss: -2.5520\n",
      "Epoch 144, Train Loss: -2.5585, Val Loss: -2.5640\n",
      "Epoch 152, Train Loss: -2.5618, Val Loss: -2.5656\n",
      "Epoch 160, Train Loss: -2.5636, Val Loss: -2.5665\n",
      "Epoch 168, Train Loss: -2.5642, Val Loss: -2.5698\n",
      "Epoch 176, Train Loss: -2.5657, Val Loss: -2.5616\n",
      "Epoch 184, Train Loss: -2.5659, Val Loss: -2.5687\n",
      "Epoch 192, Train Loss: -2.5647, Val Loss: -2.5667\n"
     ]
    }
   ],
   "source": [
    "K = 10              # number of users\n",
    "num_H = 10000          # number of training samples\n",
    "num_test = 2000            # number of testing  samples\n",
    "training_epochs = 50      # number of training epochs\n",
    "trainseed = 0              # set random seed for training set\n",
    "testseed = 7               # set random seed for test set\n",
    "print('Gaussian IC Case: K=%d, Total Samples: %d, Total Iterations: %d\\n'%(K, num_H, training_epochs))\n",
    "var_db = 10\n",
    "var = 1/10**(var_db/10)\n",
    "Xtrain, Ytrain, Atrain, wtime = generate_wGaussian(K, num_H, seed=trainseed, var_noise = var)\n",
    "X, Y, A, Y2 = generate_wGaussian(K, num_test, seed=testseed, var_noise = var)\n",
    "bl_Y = simple_greedy(X,A,Y)\n",
    "print('greedy:',np_sum_rate(X,bl_Y,A,var))\n",
    "\n",
    "print('wmmse:',np_sum_rate(X.transpose(0,2,1),Y,A,var))\n",
    "print('wmmse unweighted:',np_sum_rate(X.transpose(0,2,1),Y2,A,var))\n",
    "train_data_list = proc_data(Xtrain,Atrain)\n",
    "test_data_list = proc_data(X,A)   \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "model = IGCNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "train_loader = DataLoader(train_data_list, batch_size=64, shuffle=True,num_workers=1)\n",
    "test_loader = DataLoader(test_data_list, batch_size=2000, shuffle=False, num_workers=1)\n",
    "for epoch in range(1, 200):\n",
    "    loss1 = train()\n",
    "    if(epoch % 8 == 0):\n",
    "        loss2 = test()\n",
    "        print('Epoch {:03d}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(\n",
    "            epoch, loss1, loss2))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68144232-3389-43c5-9ed8-9c994a5bdea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
