{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fef21e18-341a-42f1-8d1d-baa0bf9fca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run IsWrls_eMMB_WmmsePowerControl.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, Sigmoid, BatchNorm1d as BN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6349e-c991-473a-b963-a03471026144",
   "metadata": {},
   "source": [
    "1. **Class Definition:**\n",
    "   - Define a class named `IGConv` which inherits from `MessagePassing`.\n",
    "\n",
    "2. **Constructor (`__init__` method):**\n",
    "   - Initialize the message passing aggregation method to 'max' and other keyword arguments.\n",
    "   - Define two multi-layer perceptrons (MLPs) named `mlp1` and `mlp2`.\n",
    "   - Call the constructor of the superclass (`MessagePassing`).\n",
    "\n",
    "3. **Parameter Initialization (`reset_parameters` method):**\n",
    "   - Reset the parameters of `mlp1` and `mlp2`.\n",
    "\n",
    "4. **Update Function (`update` method):**\n",
    "   - Concatenate the input features (`x`) and aggregated neighbor features (`aggr_out`).\n",
    "   - Pass the concatenated features through `mlp2` to obtain combined features.\n",
    "   - Concatenate the original features (`x[:,:2]`) with the combined features (`comb`) along the feature dimension.\n",
    "\n",
    "5. **Forward Pass (`forward` method):**\n",
    "   - Ensure that input features (`x`) and edge attributes (`edge_attr`) have the correct dimensions.\n",
    "   - Use the `propagate` method to perform message passing based on the given edge indices (`edge_index`), input features (`x`), and edge attributes (`edge_attr`).\n",
    "\n",
    "6. **Message Function (`message` method):**\n",
    "   - Concatenate the features of the receiving node (`x_j`) with the edge attributes (`edge_attr`) and pass them through `mlp1` to obtain aggregated messages.\n",
    "\n",
    "7. **Representation Function (`__repr__` method):**\n",
    "   - Return a string representation of the class name (`IGConv`) along with the parameters `mlp1` and `mlp2`.\n",
    "\n",
    "8. **Mathematical Formulas:**\n",
    "   - Let $x_i$ and $x_j$ be the features of the sending and receiving nodes, respectively.\n",
    "   - Let $\\text{edge\\_attr}$ be the edge attributes.\n",
    "   - $\\text{mlp1}$ and $\\text{mlp2}$ represent the multi-layer perceptron layers.\n",
    "   - $\\text{agg}$ represents the aggregated messages obtained after passing through $\\text{mlp1}$.\n",
    "   - $\\text{comb}$ represents the combined features obtained after passing through $\\text{mlp2}$.\n",
    "   - $\\text{aggr\\_out}$ represents the aggregated neighbor features after message passing.\n",
    "\n",
    "9. **Model Overview:**\n",
    "   - The class implements a graph convolutional layer using message passing.\n",
    "   - It utilizes two multi-layer perceptrons (`mlp1` and `mlp2`) to aggregate messages and update node features.\n",
    "   - The `update` method combines the original node features with the aggregated features to produce updated node representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "14577714-d387-417f-ac3a-67137e1d3345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGConv(MessagePassing):\n",
    "    def __init__(self, mlp1, mlp2, **kwargs):\n",
    "        super(IGConv, self).__init__(aggr='max', **kwargs)\n",
    "\n",
    "        self.mlp1 = mlp1\n",
    "        self.mlp2 = mlp2\n",
    "        #self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.mlp1)\n",
    "        reset(self.mlp2)\n",
    "        \n",
    "    def update(self, aggr_out, x):\n",
    "        tmp = torch.cat([x, aggr_out], dim=1)\n",
    "        comb = self.mlp2(tmp)\n",
    "        return torch.cat([x[:,:2], comb],dim=1)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "        edge_attr = edge_attr.unsqueeze(-1) if edge_attr.dim() == 1 else edge_attr\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        tmp = torch.cat([x_j, edge_attr], dim=1)\n",
    "        agg = self.mlp1(tmp)\n",
    "        return agg\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(nn={})'.format(self.__class__.__name__, self.mlp1,self.mlp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7b573-fcc3-4cbd-b877-98f88bee5648",
   "metadata": {},
   "source": [
    "1. **Input Parameters:**\n",
    "   - `channels`: List of integers representing the number of input, hidden, and output units in each layer of the MLP.\n",
    "   - `batch_norm`: Boolean indicating whether batch normalization should be applied after each linear layer.\n",
    "\n",
    "2. **Multi-Layer Perceptron (MLP) Construction:**\n",
    "   - Define a function named `MLP` that takes the list of `channels` as input and returns a sequential composition of linear layers followed by activation functions (ReLU).\n",
    "   - For each layer $i$ in the range from 1 to the length of `channels`, excluding the first element:\n",
    "     - Create a linear transformation (`Lin`) from the previous layer's output size (`channels[i - 1]`) to the current layer's output size (`channels[i]`).\n",
    "     - Apply the rectified linear unit (ReLU) activation function to introduce non-linearity.\n",
    "     - Optionally, apply batch normalization (BN) after each linear layer if `batch_norm` is set to `True`.\n",
    "\n",
    "3. **Mathematical Formulas:**\n",
    "   - Let `channels = [n_0, n_1, ..., n_L]`, where $n_0$ is the input size, $n_L$ is the output size, and $n_i$ represents the number of units in hidden layer $i$.\n",
    "   - Define `Lin(n_{i-1}, n_i)` as a linear transformation from layer $i-1$ to layer $i$.\n",
    "   - Apply the rectified linear unit (ReLU) activation function: $\\text{ReLU}(x) = \\max(0, x)$.\n",
    "   - Optionally, apply batch normalization (BN) after each linear layer to normalize the activations.\n",
    "\n",
    "4. **Model Overview:**\n",
    "   - The function constructs a multi-layer perceptron (MLP) neural network architecture with fully connected layers and ReLU activation functions.\n",
    "   - The number of layers and units in each layer are specified by the `channels` parameter.\n",
    "   - Batch normalization can be optionally included after each linear layer if `batch_norm` is set to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f50a72dc-8aa3-423e-924e-32f3d753b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(channels, batch_norm=True):\n",
    "    return Seq(*[\n",
    "        Seq(Lin(channels[i - 1], channels[i], bias = True), ReLU())#, BN(channels[i]))\n",
    "        for i in range(1, len(channels))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc83dc2-a229-47fa-ab44-f6393487b584",
   "metadata": {},
   "source": [
    "1. **Class Definition:**\n",
    "   - Define a class named `IGCNet` which inherits from `torch.nn.Module`.\n",
    "\n",
    "2. **Constructor (`__init__` method):**\n",
    "   - Initialize the module using `super(IGCNet, self).__init__()`.\n",
    "   - Define layers:\n",
    "     - `mlp1`: Multi-layer perceptron (MLP) with input size 5, hidden layer sizes [16, 32].\n",
    "     - `mlp2`: Another MLP with input size 35 and a single output neuron.\n",
    "     - `conv`: Instance of `IGConv` with `mlp1` and `mlp2` as parameters.\n",
    "\n",
    "3. **Forward Pass (`forward` method):**\n",
    "   - Accepts a `data` object containing features, edge attributes, and edge indices.\n",
    "   - Extract features (`x0`), edge attributes (`edge_attr`), and edge indices (`edge_index`) from the `data` object.\n",
    "   - Perform convolution operations using the `conv` layer:\n",
    "     - Pass the input features, edge attributes, and edge indices through the convolutional layer (`self.conv`).\n",
    "     - Repeat the convolution operation twice (`x1` and `x2`) for feature refinement.\n",
    "   - Return the final output (`out`) obtained after multiple convolution operations.\n",
    "\n",
    "4. **Mathematical Formulas:**\n",
    "   - Let $x_0$ be the input feature matrix.\n",
    "   - Let $\\text{edge\\_attr}$ be the edge attribute matrix.\n",
    "   - Let $\\text{edge\\_index}$ be the edge index tensor.\n",
    "   - $\\text{mlp1}$ and $\\text{mlp2}$ represent the multi-layer perceptron layers.\n",
    "   - $\\text{conv}$ represents the graph convolutional layer (`IGConv`).\n",
    "   - $x_1$ and $x_2$ represent the output features after the first and second convolutional layers, respectively.\n",
    "   - $\\text{out}$ represents the final output after multiple convolution operations.\n",
    "\n",
    "5. **Model Overview:**\n",
    "   - The model utilizes a graph convolutional neural network (GCN) architecture to process graph-structured data.\n",
    "   - It applies multiple graph convolution operations to refine the node features and capture complex relationships within the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "26a13750-f526-4312-a688-f33fd53b49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGCNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IGCNet, self).__init__()\n",
    "\n",
    "        self.mlp1 = MLP([5, 16, 32])\n",
    "        self.mlp2 = MLP([35, 16])\n",
    "        self.mlp2 = Seq(*[self.mlp2,Seq(Lin(16, 1, bias = True), Sigmoid())])\n",
    "        self.conv = IGConv(self.mlp1,self.mlp2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x0, edge_attr, edge_index = data.x, data.edge_attr, data.edge_index\n",
    "        x1 = self.conv(x = x0, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        x2 = self.conv(x = x1, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        #x3 = self.conv(x = x2, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        #x4 = self.conv(x = x3, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        out = self.conv(x = x2, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411b3a1-05f8-4d7e-ad6c-2229bfcc8b77",
   "metadata": {},
   "source": [
    "1. **Input Parameter:**\n",
    "   - $n$: Number of nodes in the graph.\n",
    "\n",
    "2. **Generating Complete Graph:**\n",
    "   - Initialize an empty list $\\text{adj}$ to store the adjacency list representing the graph.\n",
    "   - For each node $i$ from $0$ to $n-1$:\n",
    "     - For each node $j$ from $0$ to $n-1$:\n",
    "       - If $i \\neq j$, indicating that the edge is not a self-loop:\n",
    "         - Add the edge $(i, j)$ to the adjacency list $\\text{adj}$.\n",
    "\n",
    "3. **Output:**\n",
    "   - $\\text{adj}$: Adjacency list representing the complete graph, where each element is a pair of nodes representing an edge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2820f115-e299-45df-a11b-b570b500a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cg(n):\n",
    "    adj = []\n",
    "    for i in range(0,n):\n",
    "        for j in range(0,n):\n",
    "            if(not(i==j)):\n",
    "                adj.append([i,j])\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c73411a-f8e9-44f8-a0ee-ddef00d06321",
   "metadata": {},
   "source": [
    "1. **Input Parameters:**\n",
    "   - $H$: Channel gains matrix of shape $K \\times K$, where $K$ is the number of users.\n",
    "   - $A$: User-specific scaling factors matrix of shape $K \\times 1$.\n",
    "   - $\\text{adj}$: Adjacency list representing the graph structure.\n",
    "\n",
    "2. **Building Graph Representation:**\n",
    "   - $n = H.shape[0]$ (number of nodes in the graph)\n",
    "   - $x_1 = \\text{np.expand\\_dims}(\\text{diag}(H), axis=1)$ (expand dimensions to represent diagonal elements of $H$)\n",
    "   - $x_2 = \\text{np.expand\\_dims}(A, axis=1)$ (expand dimensions of $A$)\n",
    "   - $x_3 = \\text{np.ones}((K,1))$ (create a column vector of ones)\n",
    "   - Concatenate $x_1$, $x_2$, and $x_3$ to form feature vectors $x$\n",
    "   - Initialize an empty list $\\text{edge\\_attr}$ to store edge attributes\n",
    "   - For each edge $e$ in the adjacency list $\\text{adj}$:\n",
    "     - Compute the edge attributes based on the corresponding channel gains in $H$ and add them to $\\text{edge\\_attr}$\n",
    "   - Convert feature vectors $x$, edge indices, edge attributes, node labels $y$, and node positions $\\text{pos}$ into PyTorch tensors\n",
    "   - Create a `Data` object using the tensors to represent the graph data.\n",
    "\n",
    "3. **Output:**\n",
    "   - `data`: PyTorch `Data` object representing the graph, containing features, edge indices, edge attributes, node labels, and node positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bfb60be0-2715-4e4e-9ef1-1ae912a92a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(H,A,adj):\n",
    "    n = H.shape[0]\n",
    "    x1 = np.expand_dims(np.diag(H),axis=1)\n",
    "    x2 = np.expand_dims(A,axis=1)\n",
    "    x3 = np.ones((K,1))\n",
    "    edge_attr = []\n",
    "    \n",
    "    x = np.concatenate((x1,x2,x3),axis=1)\n",
    "    for e in adj:\n",
    "        edge_attr.append([H[e[0],e[1]],H[e[1],e[0]]])\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    edge_index = torch.tensor(adj, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    y = torch.tensor(np.expand_dims(H,axis=0), dtype=torch.float)\n",
    "    pos = torch.tensor(np.expand_dims(A,axis=0), dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(),edge_attr = edge_attr, y = y, pos = pos)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd48ee2-1dbc-429b-af23-f76ba4b9adf5",
   "metadata": {},
   "source": [
    "1. **Input Parameters:**\n",
    "   - $HH$: Channel gains matrices of shape $n \\times K \\times K$, where $n$ is the number of samples, and $K$ is the number of users.\n",
    "   - $AA$: User-specific scaling factors matrices of shape $n \\times K$, where $n$ is the number of samples, and $K$ is the number of users.\n",
    "\n",
    "2. **Processing Data:**\n",
    "   - Initialize an empty list $\\text{data\\_list}$ to store processed data for each sample.\n",
    "   - Obtain the channel gain graph ($cg$) using the function $\\text{get\\_cg}(K)$, where $K$ is the number of users.\n",
    "   - For each sample $i$ from $1$ to $n$:\n",
    "     - Build the graph representation of the channel gains and user-specific scaling factors using the function $\\text{build\\_graph}$.\n",
    "     - Append the processed data (graph representation) to $\\text{data\\_list}$.\n",
    "\n",
    "3. **Output:**\n",
    "   - $\\text{data\\_list}$: List containing the processed data for each sample, where each element represents the graph representation of the channel gains and user-specific scaling factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d9b85963-381d-4c3d-ac05-c896918f7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def proc_data(HH,AA):\n",
    "    n = HH.shape[0]\n",
    "    data_list = []\n",
    "    cg = get_cg(K)\n",
    "    for i in range(n):\n",
    "        data = build_graph(HH[i],AA[i],cg)\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642ced9-41fa-4add-a3e3-61b38ab1b820",
   "metadata": {},
   "source": [
    "1. **Input Parameters:**\n",
    "   - $H$: Channel gains matrix of shape $N \\times K \\times 1$, where $N$ is the number of samples, and $K$ is the number of users.\n",
    "   - $p$: Transmit powers matrix of shape $N \\times K \\times 1 \\times N$.\n",
    "   - $\\alpha$: User-specific weights matrix of shape $N \\times K$.\n",
    "   - $\\text{var\\_noise}$: Variance of the noise.\n",
    "\n",
    "2. **Expanding Dimensions:**\n",
    "   - Expand the dimensions of $H$ to include an additional singleton dimension at the end: $H = \\text{np.expand\\_dims}(H, axis=-1)$.\n",
    "\n",
    "3. **Calculating Received Power:**\n",
    "   - $\\text{rx\\_power} = H \\cdot p$ (element-wise multiplication)\n",
    "   - $\\text{rx\\_power} = \\sum(\\text{rx\\_power}, axis=-1)$ (sum along the last dimension)\n",
    "   - $\\text{rx\\_power} = |\\text{rx\\_power}|^2$ (square of absolute values)\n",
    "\n",
    "4. **Calculating Valid and Interference Powers:**\n",
    "   - $\\text{mask} = \\text{np.eye}(K)$ (identity matrix of size $K \\times K$)\n",
    "   - $\\text{valid\\_rx\\_power} = \\sum(\\text{rx\\_power} \\cdot \\text{mask}, axis=1)$ (sum of diagonal elements)\n",
    "   - $\\text{interference} = \\sum(\\text{rx\\_power} \\cdot (1 - \\text{mask}), axis=1) + \\text{var\\_noise}$ (sum of off-diagonal elements)\n",
    "\n",
    "5. **Calculating Rates:**\n",
    "   - $\\text{rate} = \\log\\left(1 + \\frac{\\text{valid\\_rx\\_power}}{\\text{interference}}\\right)$\n",
    "\n",
    "6. **Weighted Rates:**\n",
    "   - $\\text{w\\_rate} = \\alpha \\cdot \\text{rate}$\n",
    "\n",
    "7. **Calculating Sum Rate:**\n",
    "   - $\\text{sum\\_rate} = \\frac{1}{N} \\sum(\\sum(\\text{w\\_rate}, axis=1))$ (mean of sum of weighted rates across all samples)\n",
    "\n",
    "8. **Output:**\n",
    "   - $\\text{sum\\_rate}$: Average sum rate over all samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "493879c9-c512-4c68-8a42-94d8ced46144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_sum_rate(H,p,alpha,var_noise):\n",
    "    H = np.expand_dims(H,axis=-1)\n",
    "    K = H.shape[1]\n",
    "    N = H.shape[-1]\n",
    "    p = p.reshape((-1,K,1,N))\n",
    "    rx_power = np.multiply(H, p)\n",
    "    rx_power = np.sum(rx_power,axis=-1)\n",
    "    rx_power = np.square(abs(rx_power))\n",
    "    mask = np.eye(K)\n",
    "    valid_rx_power = np.sum(np.multiply(rx_power, mask), axis=1)\n",
    "    interference = np.sum(np.multiply(rx_power, 1 - mask), axis=1) + var_noise\n",
    "    rate = np.log(1 + np.divide(valid_rx_power, interference))\n",
    "    w_rate = np.multiply(alpha,rate)\n",
    "    sum_rate = np.mean(np.sum(w_rate, axis=1))\n",
    "    return sum_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae3c3b8-f967-4c03-b933-8687e4338f90",
   "metadata": {},
   "source": [
    "1. **Input Parameters:**\n",
    "   - $X$: Input data matrix of shape $n \\times K \\times K$, where $n$ is the number of samples, and $K$ is the number of users.\n",
    "   - $AAA$: User-specific scaling factors matrix of shape $n \\times K$.\n",
    "   - $label$: Label vector indicating the selected users for each sample.\n",
    "\n",
    "2. **Initialization:**\n",
    "   - $n = X.shape[0]$ (number of samples)\n",
    "   - $\\text{thd} = \\frac{\\sum \\text{label}}{n}$ (threshold calculated based on the label vector)\n",
    "\n",
    "3. **Greedy User Selection:**\n",
    "   - Initialize $Y$ as a zero matrix of shape $n \\times K$.\n",
    "   - For each sample $ii$ from $1$ to $n$:\n",
    "     - Extract the user-specific scaling factors $\\alpha$ for the $ii$-th sample.\n",
    "     - Calculate $H_{\\text{diag}} = \\alpha \\cdot (\\text{diagonal of } X[ii,:,:])^2$.\n",
    "     - Sort the indices of $H_{\\text{diag}}$ in descending order and store them in $xx$.\n",
    "     - Select the top $\\text{thd}$ indices from $xx$ and set the corresponding elements in $Y[ii,:]$ to 1.\n",
    "\n",
    "4. **Output:**\n",
    "   - $Y$: Matrix indicating the selected users for each sample, where each row represents a sample, and each column represents a user. A value of 1 indicates that the corresponding user is selected for the respective sample, while 0 indicates non-selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "992612c4-8913-41b5-8c77-802ade7ffb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_greedy(X,AAA,label):\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    thd = int(np.sum(label)/n)\n",
    "    Y = np.zeros((n,K))\n",
    "    for ii in range(n):\n",
    "        alpha = AAA[ii,:]\n",
    "        H_diag = alpha * np.square(np.diag(X[ii,:,:]))\n",
    "        xx = np.argsort(H_diag)[::-1]\n",
    "        for jj in range(thd):\n",
    "            Y[ii,xx[jj]] = 1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b289a-ff39-487b-afb1-4e90efc8dabe",
   "metadata": {},
   "source": [
    "1. **Input Parameters:**\n",
    "   - $K$: Number of users in the system.\n",
    "   - $num_H$: Total number of training samples.\n",
    "   - $var\\_noise$: Variance of the noise.\n",
    "   - $Pmin$: Minimum transmit power (default value is 0).\n",
    "   - $seed$: Random seed for reproducibility (default value is 2017).\n",
    "\n",
    "2. **Data Generation:**\n",
    "   - Initialize: $P_{\\text{max}} = 1$\n",
    "   - Initialize: $P_{\\text{ini}} = P_{\\text{max}} \\times \\mathbf{1}_{\\text{num\\_H} \\times K \\times 1}$ (a matrix of shape $\\text{num\\_H} \\times K \\times 1$ with all elements equal to $P_{\\text{max}}$)\n",
    "   - Generate random channel gains:\n",
    "     - Complex Gaussian Channel: $\\mathbf{CH} = \\frac{1}{\\sqrt{2}} (\\mathcal{N}(0, 1) + j \\times \\mathcal{N}(0, 1))$ (where $j$ is the imaginary unit)\n",
    "     - Magnitude of Channel Gain: $\\mathbf{H} = |\\mathbf{CH}|$\n",
    "   - Generate transmit powers using the WMMSE algorithm:\n",
    "     - $\\mathbf{Y} = \\text{WMMSE}(P_{\\text{ini}}, \\alpha, \\mathbf{H}, P_{\\text{max}}, var\\_noise)$\n",
    "   - Additionally, generate a second set of transmit powers for comparison:\n",
    "     - $\\mathbf{Y2} = \\text{WMMSE}(P_{\\text{ini}}, \\text{fake\\_a}, \\mathbf{H}, P_{\\text{max}}, var\\_noise)$\n",
    "\n",
    "3. **Output:**\n",
    "   - $\\mathbf{H}$: Random channel gains matrix of shape $\\text{num\\_H} \\times K \\times K$.\n",
    "   - $\\mathbf{Y}$: Transmitted powers matrix obtained using WMMSE algorithm of shape $\\text{num\\_H} \\times K$.\n",
    "   - $\\alpha$: Matrix of shape $\\text{num\\_H} \\times K$ containing user-specific scaling factors (currently initialized to all ones).\n",
    "   - $\\mathbf{Y2}$: Transmitted powers matrix obtained using WMMSE algorithm with a fixed scaling factor ($\\text{fake\\_a}$) of shape $\\text{num\\_H} \\times K$.\n",
    "\n",
    "This function generates random channel gains and computes the transmit powers for the given number of training samples using the WMMSE algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "701305ba-1218-4eff-9ea4-ba127d798902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wGaussian(K, num_H, var_noise=1, Pmin=0, seed=2017):\n",
    "    print('Generate Data ... (seed = %d)' % seed)\n",
    "    np. random.seed(seed)\n",
    "    Pmax = 1\n",
    "    Pini = Pmax*np.ones((num_H,K,1) )\n",
    "    #alpha = np.random.rand(num_H,K)\n",
    "    alpha = np.ones((num_H,K))\n",
    "    #alpha = np.ones((num_H,K))py\n",
    "    fake_a = np.ones((num_H,K))\n",
    "    #var_noise = 1\n",
    "    X=np.zeros((K**2,num_H))\n",
    "    Y=np.zeros((K,num_H))\n",
    "    total_time = 0.0\n",
    "    CH = 1/np.sqrt(2)*(np.random.randn(num_H,K,K)+1j*np.random.randn(num_H,K,K))\n",
    "    H=abs(CH)\n",
    "    Y = batch_WMMSE2(Pini,alpha,H,Pmax,var_noise)\n",
    "    Y2 = batch_WMMSE2(Pini,fake_a,H,Pmax,var_noise)\n",
    "    return H, Y, alpha, Y2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb125ec-7e99-406e-b1a4-1887d6912d1a",
   "metadata": {},
   "source": [
    "1. **Function Definition:**\n",
    "   - Define a function named `sr_loss` responsible for computing the loss between the model output and the ground truth.\n",
    "\n",
    "2. **Input Parameters:**\n",
    "   - $data$: Input data containing the ground truth information.\n",
    "   - $out$: Model output containing the predicted power allocations.\n",
    "   - $K$: Number of users in the system.\n",
    "\n",
    "3. **Power Allocation Extraction:**\n",
    "   - Extract the power allocations from the model output $out$.\n",
    "   - Reshape the power allocations to match the shape of the input data.\n",
    "\n",
    "4. **Calculate Received Power:**\n",
    "   - Compute the received power at each receiver by multiplying the squared absolute channel gains $abs\\_H\\_2$ with the corresponding power allocations.\n",
    "\n",
    "5. **Masking Valid Receiver Power:**\n",
    "   - Apply a masking matrix to extract the valid receiver power, considering only the diagonal elements of the received power matrix.\n",
    "\n",
    "6. **Calculate Interference:**\n",
    "   - Compute the interference by summing the received power over all interfering users and adding the noise variance $var$.\n",
    "\n",
    "7. **Compute Rate:**\n",
    "   - Calculate the achievable rate for each user using the Shannon capacity formula: $rate = \\log/_2(1 + \\frac{valid/_rx/_power}{interference})$.\n",
    "\n",
    "8. **Weighted Rate Calculation:**\n",
    "   - Multiply the achievable rate for each user by the corresponding user position $data.pos$, effectively weighting the rate by the user's position.\n",
    "\n",
    "9. **Compute Sum Rate:**\n",
    "   - Calculate the sum rate by averaging the weighted rates across all users.\n",
    "\n",
    "10. **Loss Calculation:**\n",
    "    - Negate the sum rate to obtain the loss value.\n",
    "    - Since the goal is to maximize the sum rate, negating it turns the optimization problem into a minimization problem.\n",
    "\n",
    "11. **Mathematical Formulas:**\n",
    "    - Let $ \\mathcal{D} = \\{ (\\mathbf{X}_{i}, \\mathbf{Y}_{i}) \\}_{i=1}^{\\text{num\\_test}} $ represent the input data, where $ \\mathbf{X}_i $ is the input data and $ \\mathbf{Y}_i $ is the corresponding ground truth.\n",
    "    - Let $ out $ represent the model output containing the predicted power allocations.\n",
    "    - Let $ K $ be the number of users in the system.\n",
    "    - Let $ power $ represent the extracted power allocations from the model output.\n",
    "    - Let $ abs/_H $ represent the absolute channel gains.\n",
    "    - Let $ rx/_power $ represent the received power at each receiver.\n",
    "    - Let $ mask $ represent the masking matrix to extract valid receiver power.\n",
    "    - Let $ valid/_rx/_power $ represent the valid receiver power after applying the masking matrix.\n",
    "    - Let $ interference $ represent the interference experienced by each receiver.\n",
    "    - Let $ rate $ represent the achievable rate for each user.\n",
    "    - Let $ w/_rate $ represent the weighted rate for each user based on their position.\n",
    "    - Let $ sum/_rate $ represent the sum rate computed by averaging the weighted rates across all users.\n",
    "    - The loss function is computed as the negative of the sum rate to be minimized during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c29bc12e-3224-4512-a6a3-1d176b493a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sr_loss(data, out, K):\n",
    "    power = out[:,2]\n",
    "    power = torch.reshape(power, (-1, K, 1)) \n",
    "    abs_H = data.y\n",
    "    abs_H_2 = torch.pow(abs_H, 2)  \n",
    "    rx_power = torch.mul(abs_H_2, power)\n",
    "    mask = torch.eye(K)\n",
    "    mask = mask.to(device)\n",
    "    valid_rx_power = torch.sum(torch.mul(rx_power, mask), 1)\n",
    "    interference = torch.sum(torch.mul(rx_power, 1 - mask), 1) + var\n",
    "    rate = torch.log(1 + torch.div(valid_rx_power, interference))\n",
    "    w_rate = torch.mul(data.pos,rate)\n",
    "    sum_rate = torch.mean(torch.sum(w_rate, 1))\n",
    "    loss = torch.neg(sum_rate)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65227a5d-95ed-4dab-8f22-f857a8aab2cf",
   "metadata": {},
   "source": [
    "1. **Function Definition:**\n",
    "   - Define a function named `train` responsible for training the model.\n",
    "\n",
    "2. **Training Loop:**\n",
    "   - Set the model to training mode (`model.train()`).\n",
    "   - Initialize the total loss variable (`total_loss`) to 0.\n",
    "   - Iterate over the training data batches using the `train_loader`.\n",
    "   - For each batch of data:\n",
    "     - Move the data to the appropriate device (CPU or GPU).\n",
    "     - Zero out the gradients of the optimizer (`optimizer.zero_grad()`).\n",
    "     - Pass the data through the model (`model(data)`).\n",
    "     - Calculate the loss between the model output and the ground truth using the `sr_loss` function.\n",
    "     - Backpropagate the gradients (`loss.backward()`).\n",
    "     - Update the total loss with the current batch loss multiplied by the number of graphs in the batch.\n",
    "     - Update the model parameters using the optimizer (`optimizer.step()`).\n",
    "\n",
    "3. **Loss Calculation:**\n",
    "   - The loss function (`sr_loss`) computes the loss between the model output and the ground truth.\n",
    "   - The total loss is the average loss across all batches, normalized by the total number of training samples (`num_H`).\n",
    "\n",
    "4. **Mathematical Formulas:**\n",
    "   - Let $\\mathcal{D} = \\{(\\mathbf{X}_i, \\mathbf{Y}_i)\\}_{i=1}^{\\text{num\\_H}}$ represent the training dataset, where $\\mathbf{X}_i$ is the input data and $\\mathbf{Y}_i$ is the corresponding ground truth.\n",
    "   - Let $\\text{train\\_loader}$ be the data loader that provides batches of data from $\\mathcal{D}$.\n",
    "   - Let $\\text{model}$ represent the neural network model being trained.\n",
    "   - Let $\\text{optimizer}$ be the optimization algorithm used to update the model parameters.\n",
    "   - Let $\\text{loss}(\\mathbf{Y}_{\\text{true}}, \\mathbf{Y}_{\\text{pred}})$ be the loss function used to measure the discrepancy between the true and predicted values.\n",
    "   - The total loss $\\mathcal{L}$ is calculated as the average loss across all training samples:\n",
    "     $$ \\mathcal{L} = \\frac{1}{\\text{num\\_H}} \\sum_{i=1}^{\\text{num\\_H}} \\text{loss}(\\mathbf{Y}_i, \\text{model}(\\mathbf{X}_i)) $$\n",
    "\n",
    "5. **Model Update:**\n",
    "   - The optimizer updates the model parameters using the gradients obtained from backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74cb09dc-2fca-4cfe-abfb-274f338bf4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = sr_loss(data,out,K)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / num_H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b354b-6927-4954-837e-95fe5154891d",
   "metadata": {},
   "source": [
    "1. **Function Definition:**\n",
    "   - Define a function named `test` responsible for evaluating the model's performance on the test dataset.\n",
    "\n",
    "2. **Evaluation Loop:**\n",
    "   - Set the model to evaluation mode (`model.eval()`).\n",
    "   - Initialize the total loss variable (`total_loss`) to 0.\n",
    "   - Iterate over the test data batches using the `test_loader`.\n",
    "   - For each batch of data:\n",
    "     - Move the data to the appropriate device (CPU or GPU).\n",
    "     - Disable gradient calculation to save memory and computation (`torch.no_grad()`).\n",
    "     - Pass the data through the model (`model(data)`).\n",
    "     - Calculate the loss between the model output and the ground truth using the `sr_loss` function.\n",
    "     - Update the total loss with the current batch loss multiplied by the number of graphs in the batch.\n",
    "\n",
    "3. **Loss Calculation:**\n",
    "   - The loss function (`sr_loss`) computes the loss between the model output and the ground truth.\n",
    "   - The total loss is the average loss across all batches, normalized by the total number of testing samples (`num_test`).\n",
    "\n",
    "4. **Mathematical Formulas:**\n",
    "   - Let $\\mathcal{D} = \\{(\\mathbf{X}_i, \\mathbf{Y}_i)\\}_{i=1}^{\\text{num\\_test}}$ represent the test dataset, where $\\mathbf{X}_i$ is the input data and $\\mathbf{Y}_i$ is the corresponding ground truth.\n",
    "   - Let $\\text{test\\_loader}$ be the data loader that provides batches of data from $\\mathcal{D}$.\n",
    "   - Let $\\text{model}$ represent the neural network model being evaluated.\n",
    "   - The total loss $\\mathcal{L}$ is calculated as the average loss across all testing samples:\n",
    "     $$ \\mathcal{L} = \\frac{1}{\\text{num\\_test}} \\sum_{i=1}^{\\text{num\\_test}} \\text{loss}(\\mathbf{Y}_i, \\text{model}(\\mathbf{X}_i)) $$\n",
    "\n",
    "5. **Model Evaluation:**\n",
    "   - The model's performance is evaluated by computing the loss on the test dataset, providing insight into its generalization ability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7bcefbe-49b9-43ac-980d-2e44d2970577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            loss = sr_loss(data,out,K)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / num_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9266d-f762-4b1a-b7c3-5037ee9e9b84",
   "metadata": {},
   "source": [
    "1. **Parameters:**\n",
    "   - $K$: Number of users in the system.\n",
    "   - $num_{H}$: Total number of training samples.\n",
    "   - $num_{test}$: Total number of testing samples.\n",
    "   - $training_epochs$: Number of training epochs.\n",
    "   - $trainseed$: Random seed for generating the training dataset.\n",
    "   - $testseed$: Random seed for generating the test dataset.\n",
    "   - $var\\_db$: Variance in decibels.\n",
    "   - $var$: Actual variance.\n",
    "\n",
    "2. **Data Generation:**\n",
    "   - Training Data: $(X_{\\text{train}}, Y_{\\text{train}}, A_{\\text{train}}, \\text{wtime}) = \\text{generate\\_wGaussian}(K, num_H, trainseed, var\\_noise)$\n",
    "   - Test Data: $(X, Y, A, Y2) = \\text{generate\\_wGaussian}(K, num_test, testseed, var\\_noise)$\n",
    "\n",
    "3. **Algorithm Execution:**\n",
    "   - Simple Greedy Algorithm: $\\text{bl\\_Y} = \\text{simple\\_greedy}(X, A, Y)$\n",
    "\n",
    "4. **Data Processing:**\n",
    "   - Training Data Processing: $\\text{train\\_data\\_list} = \\text{proc\\_data}(X_{\\text{train}}, A_{\\text{train}})$\n",
    "   - Test Data Processing: $\\text{test\\_data\\_list} = \\text{proc\\_data}(X, A)$\n",
    "\n",
    "5. **Model Definition:**\n",
    "   - $\\text{device} = \\text{cuda if available else cpu}$\n",
    "   - Neural Network Model: $\\text{model} = \\text{IGCNet}().\\text{to(device)}$\n",
    "\n",
    "6. **Training:**\n",
    "   - Optimizer: $\\text{optimizer} = \\text{Adam}(\\text{model.parameters()}, lr=0.001)$\n",
    "   - Learning Rate Scheduler: $\\text{scheduler} = \\text{StepLR}(\\text{optimizer}, step\\_size=20, gamma=0.9)$\n",
    "   - Training Data Loader: $\\text{train\\_loader} = \\text{DataLoader}(\\text{train\\_data\\_list}, batch\\_size=64, shuffle=True, num\\_workers=1)$\n",
    "   - Test Data Loader: $\\text{test\\_loader} = \\text{DataLoader}(\\text{test\\_data\\_list}, batch\\_size=2000, shuffle=False, num\\_workers=1)$\n",
    "\n",
    "7. **Training Loop:**\n",
    "   - For each epoch in range(1, 200):\n",
    "     - Training: $\\text{loss1} = \\text{train}()$\n",
    "     - If epoch is a multiple of 8:\n",
    "       - Testing: $\\text{loss2} = \\text{test}()$\n",
    "       - Print epoch number, training loss, and validation loss.\n",
    "     - Learning Rate Scheduler Step: $\\text{scheduler.step()}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "75430ee7-7f65-470f-99b4-3bad165adbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian IC Case: K=10, Total Samples: 10000, Total Iterations: 50\n",
      "\n",
      "Generate Data ... (seed = 0)\n",
      "Generate Data ... (seed = 7)\n",
      "greedy: 2.8668466257428618\n",
      "wmmse: 3.841383699946745\n",
      "wmmse unweighted: 3.841383699946745\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "K = 10                                # number of users\n",
    "num_H = 10000                         # number of training samples\n",
    "num_test = 2000                       # number of testing  samples\n",
    "training_epochs = 50                  # number of training epochs\n",
    "trainseed = 0                         # set the random seed for the training set\n",
    "testseed = 7                          # set random seed for test set\n",
    "print('Gaussian IC Case: K=%d, Total Samples: %d, Total Iterations: %d\\n'%(K, num_H, training_epochs))\n",
    "var_db = 10\n",
    "var = 1/10**(var_db/10)\n",
    "Xtrain, Ytrain, Atrain, wtime = generate_wGaussian(K, num_H, seed=trainseed, var_noise = var)\n",
    "X, Y, A, Y2 = generate_wGaussian(K, num_test, seed=testseed, var_noise = var)\n",
    "bl_Y = simple_greedy(X,A,Y)\n",
    "print('greedy:',np_sum_rate(X,bl_Y,A,var))\n",
    "print('wmmse:',np_sum_rate(X.transpose(0,2,1),Y,A,var))\n",
    "print('wmmse unweighted:',np_sum_rate(X.transpose(0,2,1),Y2,A,var))\n",
    "train_data_list = proc_data(Xtrain,Atrain)\n",
    "test_data_list = proc_data(X,A)   \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "model = IGCNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "train_loader = DataLoader(train_data_list, batch_size=64, shuffle=True,num_workers=1)\n",
    "test_loader  = DataLoader(test_data_list, batch_size=2000, shuffle=False, num_workers=1)\n",
    " for epoch in range(1, 200):\n",
    "     loss1 = train()\n",
    "     if(epoch % 8 == 0):\n",
    "         loss2 = test()\n",
    "         print('Epoch {:03d}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(\n",
    "             epoch, loss1, loss2))\n",
    "     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc9dd7-ae8d-4a77-8509-9b7dd9b68bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
