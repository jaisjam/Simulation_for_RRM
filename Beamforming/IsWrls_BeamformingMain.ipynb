{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd243b12-87b3-4c24-b6a5-1317af4d36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebooks for the submodule. \n",
    "%run IsWrls_WrlsNWGenerater.ipynb\n",
    "%run IsWrls_WrlsNW_wmmse.ipynb\n",
    "\n",
    "import numpy as np  # import numpy\n",
    "import time         # import time functions \n",
    "import torch        # Import the torch library\n",
    "\n",
    "from torch_geometric.data import Data        # Import the Data class from the torch_geometric.data module\n",
    "from torch_geometric.loader import DataLoader  # Import the DataLoader class from the torch_geometric.loader module\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, Sigmoid, BatchNorm1d as BN\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import MessagePassing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de2fe4-7487-4cb8-8a84-a40620cfcb1e",
   "metadata": {},
   "source": [
    "## Class init_parameters\n",
    "* In wireless network simulations, it's crucial to define parameters that require the behaviour of the simulated environment. This class serves as a container for such parameters, making it easier to manage and access them throughout the simulation process.\n",
    "\n",
    ">**1. Wireless Network Settings:**\n",
    ">>Parameters like n_links, field_length, bandwidth, carrier_f, etc., define characteristics of the wireless network, such as the number of links, field dimensions, frequency band, and transmission characteristics.\n",
    "\n",
    ">**2. Antenna Configuration:**\n",
    ">>Parameters such as tx_height, rx_height, antenna_gain_decibel, etc., specify details about the antennas used in the simulation, including their heights, gains, and transmission powers.\n",
    "\n",
    ">**3. Noise Parameters:**\n",
    ">>Parameters like noise_density_milli_decibel, input_noise_power, output_noise_power, SNR_gap_dB, etc., represent the noise characteristics of the wireless channel, including noise density, signal-to-noise ratio (SNR) gaps, and noise powers.\n",
    "\n",
    ">**4. 2D Occupancy Grid Settings:**\n",
    ">>Parameters such as cell_length, N_antennas, maxrx, minrx, n_grids, etc., define the grid-based representation of the simulation environment, including cell dimensions, antenna configurations within cells, and grid resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4364aba7-fc24-4fe7-ace2-896bb92ffbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class init_parameters():\n",
    "    def __init__(self, train_K, Nt):\n",
    "        \"\"\"\n",
    "        Initializes parameters for a wireless network simulation.\n",
    "\n",
    "        Args:\n",
    "        train_K (int): Number of links and receivers in the network.\n",
    "        Nt (int): Number of antennas.\n",
    "        \"\"\"\n",
    "        # Wireless network settings\n",
    "        self.n_links = train_K  # Number of links in the network\n",
    "        self.n_receiver = train_K  # Number of receivers in the network\n",
    "        self.field_length = 1000  # Length of the field where the network operates\n",
    "        self.shortest_directLink_length = 2  # Shortest length of direct links\n",
    "        self.longest_directLink_length = 65  # Longest length of direct links\n",
    "        self.shortest_crossLink_length = 1  # Shortest length of cross-links\n",
    "        self.bandwidth = 5e6  # Bandwidth of the network\n",
    "        self.carrier_f = 2.4e9  # Carrier frequency\n",
    "        self.tx_height = 1.5  # Height of the transmitter\n",
    "        self.rx_height = 1.5  # Height of the receiver\n",
    "        self.antenna_gain_decibel = 2.5  # Antenna gain in decibels\n",
    "        self.tx_power_milli_decibel = 40  # Transmit power in milliwatts (in decibels)\n",
    "        self.tx_power = 1  # Transmit power (commented out calculation)\n",
    "        self.noise_density_milli_decibel = -169  # Noise density (in decibels)\n",
    "        self.input_noise_power = 1  # Input noise power (commented out calculation)\n",
    "        self.output_noise_power = 1  # Output noise power\n",
    "        self.SNR_gap_dB = 6  # Signal-to-noise ratio gap (in decibels)\n",
    "        self.SNR_gap = 1  # Signal-to-noise ratio gap (commented-out calculation)\n",
    "        self.setting_str = \"{}_links_{}X{}_{}_{}_length\".format(self.n_links, self.field_length, self.field_length, self.shortest_directLink_length, self.longest_directLink_length)\n",
    "        # 2D occupancy grid setting\n",
    "        self.cell_length = 5  # Length of each cell in the grid\n",
    "        self.N_antennas = Nt  # Number of antennas\n",
    "        self.maxrx = 2  # Maximum number of receivers\n",
    "        self.minrx = 1  # Minimum number of receivers\n",
    "        self.n_grids = 1  # Number of grids (commented-out calculation)\n",
    "\n",
    "# Testing for the class definition\n",
    "# train_K = 30  # Number of links and receivers\n",
    "# Nt = 10  # Number of antennas\n",
    "#train_config = init_parameters(train_K, Nt)  # Initializing parameters\n",
    "#print(f\"Number of links for the network is {train_config.n_links} and the number of receivers is {train_config.n_receiver}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c3915-efc4-42b0-9f05-dd1196874922",
   "metadata": {},
   "source": [
    "The **MLP** function constructs a **multi-layer perceptron (MLP)** neural network using PyTorch's **Sequential module**.\n",
    "Let as **MLP(x)** represent the output of the MLP neural network for input *x*. The function consists of multiple linear transformation and activation layers: <br>\n",
    "\n",
    "**1. Linear Transformation:** \n",
    ">For each layer $l_{l}$ in the MLP with input dimension $d_{l-1}$ and output dimension $d_{l}$, a linear transformation is applied: <br><br>\n",
    "$ \\boxed{\\mathrm{Linear}(X(t)) = \\mathrm W_{l}\\cdot  \\mathrm X(t) + \\mathrm B_{l}} $. <br><br>\n",
    " where $ W_{l} $ is the weight matrix and $ B_{l} $ is the bias vector for layer $ l $. <br>\n",
    "\n",
    "**2. Activation Function (ReLU):**\n",
    ">The **Rectified Linear Unit (ReLU)** activation function is a simple yet widely used non-linear activation function in neural networks. It's defined mathematically as: <br><br>\n",
    "$ \\boxed{ReLU(x)=Max(0,x)} $ <br><br>\n",
    ">In other words, ReLU sets all negative values in the input tensor x to zero, while leaving positive values unchanged. This leads to sparse >activation, which helps alleviate the vanishing gradient problem during training and encourages sparse representations in neural networks.\n",
    "In neural network architectures, ReLU is typically used as the activation function in hidden layers, while other activation functions like softmax or sigmoid are used in output layers for specific tasks like classification or regression.\n",
    "\n",
    "**Batch Normalization (optional):**\n",
    "If batch normalization is enabled, a Batch Normalization (BN) layer is applied after each linear transformation to normalize the activations:<br><br>\n",
    ">* **Normalization:** &nbsp; For each mini-batch during training, batch normalization normalizes the activations of each layer by subtracting the mini-batch mean and dividing by the mini-batch standard deviation. This is performed independently for each feature dimension. <br>\n",
    ">* **Scaling and Shifting:** &nbsp; After normalization, the activations are scaled and shifted using learnable parameters (gamma and beta) to allow the network to learn the optimal scale and shift for each feature dimension. <br>\n",
    "\n",
    ">Mathematically, batch normalization can be defined as follows: <br><br>\n",
    "$ \\boxed{BN(t) = \\gamma \\frac{t-μ}{\\sigma} + \\beta} $\n",
    ">Where:\n",
    ">* t => the input tensor.\n",
    ">* μ => the mean of the mini-batch.\n",
    ">* σ => the standard deviation of the mini-batch.\n",
    ">* γ => learnable scale\n",
    ">* β => shift parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "736dd0d2-ec63-478a-b3a9-416115ff079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(channels, batch_norm=True):\n",
    "    return Seq(*[\n",
    "        Seq(Lin(channels[i - 1], channels[i]), ReLU())#, BN(channels[i])\n",
    "        for i in range(1, len(channels))\n",
    "    ])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e42aef4-3b18-4c98-b3fe-db00b8a23a4f",
   "metadata": {},
   "source": [
    "*This **class IGConv**, appears to be a **Graph Neural Network (GNN)** layer implemented using **PyTorch's** Geometric library. Let's break down its components and functionality:*\n",
    "\n",
    "> **Initialization:**\n",
    ">> Inherits from MessagePassing, which is a base class provided by PyTorch Geometric for implementing graph neural network layers. Accepts two parameters mlp1 and mlp2, which are presumably multi-layer perceptron (MLP) modules used for message passing and node feature update, respectively.Calls the superclass constructor with aggr='max' and any additional keyword arguments.\n",
    "\n",
    "> **Reset Parameters:**\n",
    ">> Defines a method reset_parameters to initialize the parameters of the MLP modules.\n",
    "\n",
    "> **Update Function:**\n",
    ">> Defines an update method that computes the new node features based on aggregated messages and the current node features. Concatenates the aggregated messages (aggr_out) with the current node features (x). Passes the concatenated tensor through mlp2 and performs some normalization (nor) on the output. Concatenates the normalized output with a subset of the original node features. Returns the updated node features.\n",
    "\n",
    "> **Forward Function:**\n",
    ">> Defines a forward method that takes node features (x), edge indices (edge_index), and edge attributes (edge_attr) as input. Reshapes the input tensors if they are one-dimensional. Calls the propagate method inherited from MessagePassing, which applies message passing to the graph.\n",
    "\n",
    "> **Message Function:**\n",
    ">> Defines a message method that computes messages sent from source nodes to target nodes. Concatenates the target node features (x_j) with edge attributes (edge_attr). Passes the concatenated tensor through mlp1 to compute the messages.\n",
    "\n",
    "> **Representation Function:**\n",
    ">>Overrides the __repr__ method to provide a string representation of the class instance, showing the class name and the parameters mlp1 and mlp2.\n",
    "\n",
    "*Overall, this class defines a graph convolutional layer (IGConv) that utilizes two MLP modules (mlp1 and mlp2) for message passing and node feature update in a graph neural network. It follows the message-passing paradigm commonly used in GNNs, where information is exchanged between neighboring nodes to update node representations.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a019163-a16b-44ce-b6c6-e7a1aeadf6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGConv(MessagePassing):\n",
    "    def __init__(self, mlp1, mlp2, **kwargs):\n",
    "        super(IGConv, self).__init__(aggr='max', **kwargs)\n",
    "\n",
    "        self.mlp1 = mlp1\n",
    "        self.mlp2 = mlp2\n",
    "        #self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.mlp1)\n",
    "        reset(self.mlp2)\n",
    "        \n",
    "    def update(self, aggr_out, x):\n",
    "        tmp = torch.cat([x, aggr_out], dim=1)\n",
    "        comb = self.mlp2(tmp)\n",
    "        nor = torch.sqrt(torch.sum(torch.mul(comb,comb),axis=1))\n",
    "        nor = nor.unsqueeze(axis=-1)\n",
    "        comp1 = torch.ones(comb.size(), device=device)\n",
    "        comb = torch.div(comb,torch.max(comp1,nor) )\n",
    "        return torch.cat([comb, x[:,:2*Nt]],dim=1)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "        edge_attr = edge_attr.unsqueeze(-1) if edge_attr.dim() == 1 else edge_attr\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        tmp = torch.cat([x_j, edge_attr], dim=1)\n",
    "        agg = self.mlp1(tmp)\n",
    "        return agg\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(nn={})'.format(self.__class__.__name__, self.mlp1,self.mlp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fe01a-b012-447f-8824-66d8a7ecadd6",
   "metadata": {},
   "source": [
    "*This code defines a neural network model called IGCNet using PyTorch.*\n",
    "\n",
    "**Initialization:** \n",
    "* The IGCNet class is defined, which is a subclass of torch.nn.Module, indicating that it's a neural network model.\n",
    "* Inside the **\\__init__** method, the initial setup of the model is defined.\n",
    "* The **super(IGCNet, self).\\__init__()** line initializes the parent class (torch.nn.Module) to ensure proper inheritance.\n",
    "\n",
    "**MLP Modules Initialization:**\n",
    "* Two multi-layer perceptron (MLP) modules (**self.mlp1** and **self.mlp2**) are initialized using the MLP function with specific layer configurations.\n",
    "* **self.mlp1** has layers with input size 6*Nt, output size 64, and 64.\n",
    "* **self.mlp2** has layers with input size 64+4*Nt and output size 32.\n",
    "\n",
    "**Sequential Composition:**\n",
    "* The self.mlp2 is modified by adding a linear layer (Lin) with output size 2*Nt at the end using the Seq function.\n",
    "\n",
    "**IGConv Layer Initialization:**\n",
    "* An instance of the IGConv class is created, which is presumably a graph convolutional layer.\n",
    "* This layer is initialized with the previously defined MLP modules (self.mlp1 and self.mlp2).\n",
    "\n",
    "**Forward Pass:**\n",
    "* The forward method defines how data flows through the model during the forward pass.\n",
    "* Input data (x0, edge_attr, edge_index) is passed through the graph convolutional layer (self.conv) three times (x1, x2, out).\n",
    "* Each time, the output of the previous pass is used as input (x) along with edge attributes and indices.\n",
    "\n",
    "*In summary, the IGCNet model consists of two MLP modules (mlp1 and mlp2), which are then used within a graph convolutional layer (IGConv). During the forward pass, input data is passed through the graph convolutional layer multiple times to generate the final output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31979646-f718-4b12-8959-acd2827c457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGCNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IGCNet, self).__init__()\n",
    "\n",
    "        self.mlp1 = MLP([6*Nt, 64, 64])\n",
    "        self.mlp2 = MLP([64+4*Nt, 32])\n",
    "        self.mlp2 = Seq(*[self.mlp2,Seq(Lin(32, 2*Nt))])\n",
    "        self.conv = IGConv(self.mlp1,self.mlp2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x0, edge_attr, edge_index = data.x, data.edge_attr, data.edge_index\n",
    "        x1 = self.conv(x = x0, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        x2 = self.conv(x = x1, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        out = self.conv(x = x2, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc70d9-ae6a-4229-93df-f9f274075bf5",
   "metadata": {},
   "source": [
    "### Normalization Process\n",
    "\n",
    "*This equation represents the normalization process for both training and test data. Each element of the channel matrix is normalized separately using its respective mean and standard deviation. Finally, the normalized diagonal and off-diagonal elements are combined to obtain the final normalized channel matrix.*\n",
    "\n",
    "Given a channel matrix $H$ of size $M \\times M$, where $M$ is the number of antennas:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - $H$ is the input channel matrix.\n",
    "   - $N_t$ is the number of antennas.\n",
    "\n",
    "2. **Mask Creation**:\n",
    "   - Let mask $\\text{M}_{ij}$ be the mask indicating whether element $H_{ij}$ is on the diagonal or off-diagonal. $\\text{mask}_{ij} = 1$ if $i = j$, and $\\text{mask}_{ij} = 0$ otherwise.\n",
    "\n",
    "3. **Diagonal Normalization**:\n",
    "   - Calculate the mean $\\mu_{\\text{diag}}$ and standard deviation $\\sigma_{\\text{diag}}$ of the diagonal elements:\n",
    "     $\\boxed{\\mu_{\\text{diag}} = \\frac{1}{N_t} \\sum_{i=1}^{N_t} H_{ii}}$ <br>\n",
    "     $\\boxed{\\sigma_{\\text{diag}} = \\sqrt{\\frac{1}{N_t} \\sum_{i=1}^{N_t} (H_{ii} - \\mu_{\\text{diag}})^2}}$ \n",
    "   - Normalize the diagonal elements:\n",
    "     $\\boxed{\\tilde{H}_{ij} = \\frac{H_{ij} - \\mu_{\\text{diag}}}{\\sigma_{\\text{diag}}} \\quad \\text{if} \\quad i = j}$\n",
    "\n",
    "4. **Off-Diagonal Normalization**:\n",
    "   - Calculate the mean $\\mu_{\\text{off-diag}}$ and standard deviation $\\sigma_{\\text{off-diag}}$ of the off-diagonal elements:\n",
    "     $\\mu_{\\text{off-diag}} = \\frac{1}{M - N_t} \\sum_{i=1}^{M} \\sum_{j=1}^{M} (1 - \\text{mask}_{ij}) H_{ij}$\n",
    "     $\\sigma_{\\text{off-diag}} = \\sqrt{\\frac{1}{M - N_t} \\sum_{i=1}^{M} \\sum_{j=1}^{M} (1 - \\text{mask}_{ij}) (H_{ij} - \\mu_{\\text{off-diag}})^2}$$\n",
    "   - Normalize the off-diagonal elements:\n",
    "     $\\tilde{H}_{ij} = \\frac{H_{ij} - \\mu_{\\text{off-diag}}}{\\sigma_{\\text{off-diag}}} \\quad \\text{if} \\quad i \\neq j$\n",
    "\n",
    "5. **Combining Diagonal and Off-Diagonal**:\n",
    "   - The final normalized channel matrix $\\tilde{H}$ is obtained by combining the normalized diagonal and off-diagonal elements:\n",
    "     $ \\tilde{H}_{ij} = \\begin{cases} \\frac{H_{ij} - \\mu_{\\text{diag}}}{\\sigma_{\\text{diag}}} & \\text{if } i = j \\\\ \\frac{H_{ij} - \\mu_{\\text{off-diag}}}{\\sigma_{\\text{off-diag}}} & \\text{if } i \\neq j \\end{cases} $\n",
    "\n",
    "This Markdown representation provides a concise summary of the normalization process using mathematical equations with summation notation. Each step is explained in detail, making it easier to understand and implement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a3585e9-46d3-4256-bed3-e3fae5aef7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(train_data, test_data, general_para):\n",
    "    Nt = general_para.N_antennas\n",
    "    \n",
    "    tmp_mask = np.expand_dims(np.eye(train_K),axis=-1)\n",
    "    tmp_mask = [tmp_mask for i in range(Nt)]\n",
    "    mask = np.concatenate(tmp_mask,axis=-1)\n",
    "    mask = np.expand_dims(mask,axis=0)\n",
    "    \n",
    "    train_copy = np.copy(train_data)\n",
    "    diag_H = np.multiply(mask,train_copy)\n",
    "    diag_mean = np.sum(diag_H/Nt)/train_layouts/train_K\n",
    "    diag_var = np.sqrt(np.sum(np.square(diag_H))/train_layouts/train_K/Nt)\n",
    "    tmp_diag = (diag_H - diag_mean)/diag_var\n",
    "\n",
    "    off_diag = train_copy - diag_H\n",
    "    off_diag_mean = np.sum(off_diag/Nt)/train_layouts/train_K/(train_K-1)\n",
    "    off_diag_var = np.sqrt(np.sum(np.square(off_diag))/Nt/train_layouts/train_K/(train_K-1))\n",
    "    tmp_off = (off_diag - off_diag_mean)/off_diag_var\n",
    "    tmp_off_diag = tmp_off - np.multiply(tmp_off,mask)\n",
    "    \n",
    "    norm_train = np.multiply(tmp_diag,mask) + tmp_off_diag\n",
    "    \n",
    "    # normlize test\n",
    "    tmp_mask = np.expand_dims(np.eye(test_K),axis=-1)\n",
    "    tmp_mask = [tmp_mask for i in range(Nt)]\n",
    "    mask = np.concatenate(tmp_mask,axis=-1)\n",
    "    mask = np.expand_dims(mask,axis=0)\n",
    "    \n",
    "    test_copy = np.copy(test_data)\n",
    "    diag_H = np.multiply(mask,test_copy)\n",
    "    tmp_diag = (diag_H - diag_mean)/diag_var\n",
    "    \n",
    "    off_diag = test_copy - diag_H\n",
    "    tmp_off = (off_diag - off_diag_mean)/off_diag_var\n",
    "    tmp_off_diag = tmp_off - np.multiply(tmp_off,mask)\n",
    "    \n",
    "    norm_test = np.multiply(tmp_diag,mask) + tmp_off_diag\n",
    "    print(diag_mean, diag_var, off_diag_mean, off_diag_var)\n",
    "    return norm_train, norm_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6838e-3860-487d-96ff-bc1ceee980f4",
   "metadata": {},
   "source": [
    "### Building Graph from Channel State Information (CSI)\n",
    "\n",
    "The `build_graph` function constructs a graph representation from CSI, distances, and other parameters.\n",
    "\n",
    "#### Input Parameters:\n",
    "\n",
    "- `CSI`: Channel State Information matrix.\n",
    "- `dist`: Distance matrix.\n",
    "- `K`: Number of users.\n",
    "- `threshold`: Threshold value for distance filtering.\n",
    "\n",
    "#### Derivation:\n",
    "\n",
    "- `n`  : CSI matrix $[H]_{r \\times c}$ size\n",
    "- `N_t`: number of antennas.\n",
    "\n",
    "##### Extract real and imaginary parts:\n",
    "$\\boxed{ X = (\\text{[CSI]} \\times \\text{[I]})^{T} }               $\n",
    "- $Q = \\text{imag}(X)$, &nbsp; $I = \\text{real}(X)                $<br> <br>\n",
    "\n",
    "##### Create feature vector:\n",
    "$\\boxed{ X = [\\frac{1}{\\sqrt{N_t}}(\\mathbf{1}_{n,2N_t}), Q, I] }  $\n",
    "\n",
    "##### Filter distances:\n",
    "$\\boxed{ \\text{D}         = \\text{d} + (1000 \\cdot (\\text{[d]}\\times \\text{[I]}))^{T} } $ <br><br>\n",
    "\n",
    "$\\boxed{\\text{D}_{ij}     = \\begin{cases}\n",
    "                            0 & \\text{if } \\text{D}_{ij} > \\text{threshold} \n",
    "                            \\\\ \n",
    "                            \\text{D}_{ij} & \\text{otherwise} \\end{cases} }              $ <br><br>\n",
    "                            \n",
    "$\\boxed{\\text{E}          = \\begin{cases}\n",
    "                            [\\text{i}, \\text{j}] & \\text{if} \\text{D}_{i,j} \\neq 0\n",
    "                            \\\\\n",
    "                            0 & \\text{otherwise} \\end{cases}} $ <br><br>\n",
    "                            \n",
    "$\\boxed{\\text{A}          = [\\text{E(1)}, \\text{E(0)}]}              $ <br><br>\n",
    "$\\boxed{\\text{HH} = \\text{I} + \\text{j} \\text{Q}  }         $ <br><br>\n",
    "$\\boxed{\\text{Data} = [\\text{X}, \\text{E}, \\text{A}]}                 $\n",
    "- `X`: Complex data from CSI\n",
    "- `E`: Edge Indeces\n",
    "- `A`: Attributes of the edge index\n",
    "          \n",
    "This provides a mathematical overview of the `build_graph` function, explaining each step with equations and operations performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b565e533-d557-4fd0-b834-bc84239089e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(CSI, dist, norm_csi_real, norm_csi_imag, K, threshold):\n",
    "    n = CSI.shape[0]\n",
    "    Nt = CSI.shape[2]\n",
    "    x1 = np.array([CSI[ii,ii,:] for ii in range(K)])\n",
    "    x2 = np.imag(x1)\n",
    "    x1 = np.real(x1)\n",
    "    x3 = 1/np.sqrt(Nt)*np.ones((n,2*Nt))\n",
    "    \n",
    "    x = np.concatenate((x3,x1,x2),axis=1)\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    \n",
    "    dist2 = np.copy(dist)\n",
    "    mask = np.eye(K)\n",
    "    diag_dist = np.multiply(mask,dist2)\n",
    "    dist2 = dist2 + 1000 * diag_dist\n",
    "    dist2[dist2 > threshold] = 0\n",
    "    attr_ind = np.nonzero(dist2)\n",
    "    \n",
    "    edge_attr_real = norm_csi_real[attr_ind]\n",
    "    edge_attr_imag = norm_csi_imag[attr_ind]\n",
    "    \n",
    "    edge_attr = np.concatenate((edge_attr_real,edge_attr_imag), axis=1)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "    attr_ind = np.array(attr_ind)\n",
    "    adj = np.zeros(attr_ind.shape)\n",
    "    adj[0,:] = attr_ind[1,:]\n",
    "    adj[1,:] = attr_ind[0,:]\n",
    "    edge_index = torch.tensor(adj, dtype=torch.long)\n",
    "    \n",
    "    H1 = np.expand_dims(np.real(CSI),axis=-1)\n",
    "    H2 = np.expand_dims(np.imag(CSI),axis=-1)\n",
    "    HH = np.concatenate((H1,H2),axis=-1)\n",
    "    y = torch.tensor(np.expand_dims(HH,axis=0), dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index.contiguous(),edge_attr = edge_attr, y = y)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f0233f5-0751-4103-a271-51cfcf9c3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_data(HH, dists, norm_csi_real, norm_csi_imag, K):\n",
    "    n = HH.shape[0]\n",
    "    data_list = []\n",
    "    for i in range(n):\n",
    "        data = build_graph(HH[i,:,:,:],dists[i,:,:], norm_csi_real[i,:,:,:], norm_csi_imag[i,:,:,:], K,500)\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30aba31-b281-4c20-9cab-4fb7244bcfad",
   "metadata": {},
   "source": [
    "The function iterates over each instance of the CSI matrix, and for each instance iteration, it computes Y using the function np_WMMSE_vectornp_WMMSE_vector, which calculates the Weighted Minimum Mean Squared Error (WMMSE) solution for the given CSI and noise variance.\n",
    "The output Y contains the result of the WMMSE algorithm applied to each instance of the CSI matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cfbed55-fe99-4e44-9158-6e7d81ecbf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_wmmse(csis,var_noise):\n",
    "    Nt = test_config.N_antennas\n",
    "    K = test_config.n_receiver\n",
    "    n = csis.shape[0]\n",
    "    Y = np.zeros( (n,K,Nt),dtype=complex)\n",
    "    Pini = 1/np.sqrt(Nt)*np.ones((K,Nt),dtype=complex)\n",
    "    for ii in range(n):\n",
    "        Y[ii,:,:] = np_WMMSE_vector(np.copy(Pini), csis[ii,:,:,:], 1, var_noise)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed2972-d33c-4403-87ec-76d1ab6732f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3955b069-6150-4dff-8d42-ca98d6393fdd",
   "metadata": {},
   "source": [
    "The `power_check` function checks the power constraint for a given power allocation matrix.\n",
    "#### 1. Input Parameters:\n",
    "   - `p`: Power allocation matrix.\n",
    "\n",
    "#### 2.Compute Power Constraints:\n",
    "   - Compute the squared norm of each row in the power allocation matrix: <br>\n",
    "     $ \\boxed{\\text{p} = \\sum_{i=1}^{n} p_i^{2}} $\n",
    "   - Count the number of rows where the squared norm exceeds the threshold of `1.1`.\n",
    "\n",
    "This function is used to verify whether the power allocation matrix satisfies the power constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c76eaf7f-5f59-480f-8d9b-31ad5dbf28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_check(p):\n",
    "    n = p.shape[0]\n",
    "    pp = np.sum(np.square(p),axis=1)\n",
    "    print(np.sum(pp>1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ada637-4cfd-4073-8aa4-0876ff1699c1",
   "metadata": {},
   "source": [
    "### Sum Rate Loss Function\n",
    "\n",
    "The `sr_loss` function computes the sum rate loss based on the received power and interference of the channels.\n",
    "\n",
    "#### Input Parameters:\n",
    "- `H_1`: Tensor representing the channel coefficients for the first antenna.\n",
    "- `H_2`: Tensor representing the channel coefficients for the second antenna.\n",
    "- `p_1`: Power allocation tensor for the first antenna.\n",
    "- `p_2`: Power allocation tensor for the second antenna.\n",
    "- `K`: Number of users.\n",
    "- `N`: Number of antennas.\n",
    "\n",
    "#### Procedure:\n",
    "\n",
    "1. **Received Power Calculations**:\n",
    "   - Compute received power for the first antenna:  <br><br>\n",
    "     $ \\boxed{\\text{rx\\_power1} = \\sum_{i=1}^{K} \\sum_{j=1}^{N} H_{i,j} \\cdot p1_{i,j} } $ <br><br>\n",
    "   - Compute received power for the second antenna: <br><br>\n",
    "     $ \\boxed{\\text{rx\\_power2} = \\sum_{i=1}^{K} \\sum_{i=1}^{N} H_{i,j} \\cdot p2_{i,j} } $ <br><br>\n",
    "   - Compute cross-channel interference: <br><br>\n",
    "     $ \\boxed{\\text{rx\\_power3} = \\sum_{i=1}^{K} \\sum_{j=1}^{N} H_{i,j} \\cdot p1_{i,j} } $ <br><br>\n",
    "     $ \\boxed{\\text{rx\\_power4} = \\sum_{i=1}^{K} \\sum_{j=1}^{N} H_{i,j} \\cdot p2_{i,j} } $ <br><br>\n",
    "\n",
    "2. **Received Power and Interference**:\n",
    "   - Combine received power and interference: <br><br>\n",
    "     $ \\boxed{\\text{signal\\_power} = (\\text{rx\\_power1} - \\text{rx\\_power2})^2 + (\\text{rx\\_power3} + \\text{rx\\_power4})^2} $ <br><br>\n",
    "   - Separate valid received power and interference using a mask: <br><br>\n",
    "     $ \\boxed{\\text{abs\\_power}   = \\sum_{i=1}^{K} (\\text{signal\\_power}_{i,i})} $ <br><br>\n",
    "     $ \\boxed{\\text{interference} = \\sum_{i=1}^{K} \\sum_{j = 0}^{N} (\\text{signal\\_power}_{i,j \\neq i}) + 1} $ <br><br>\n",
    "\n",
    "3. **Rate Calculation**:\n",
    "   - Compute the achievable rate for each user: <br>\n",
    "     $ \\boxed{\\text{rate}_i = \\log_2 \\left(1 + \\frac{\\text{abs\\_power}_i}{\\text{interference}_i} \\right)} $ <br><br>\n",
    "   - Compute the sum rate: <br><br>\n",
    "     $ \\boxed{\\text{sum\\_rate} = \\frac{1}{K} \\sum_{i=1}^{K} (\\text{rate}_i)} $ <br><br>\n",
    "\n",
    "4. **Loss Calculation**:\n",
    "   - Compute the negative sum rate loss: <br><br>\n",
    "     $ \\boxed{\\text{loss} = -\\text{sum\\_rate} } $ <br><br>\n",
    "\n",
    "#### Output:\n",
    "\n",
    "The function returns the sum rate loss as the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f90daf51-9bf7-4ac4-ac1d-fef5891ba484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sr_loss(data,p,K,N):\n",
    "    # H1 K*K*N\n",
    "    # p1 K*N\n",
    "    H1 = data.y[:,:,:,:,0]\n",
    "    H2 = data.y[:,:,:,:,1]\n",
    "    p1 = p[:,:N]\n",
    "    p2 = p[:,N:2*N]\n",
    "    p1 = torch.reshape(p1,(-1,K,1,N))\n",
    "    p2 = torch.reshape(p2,(-1,K,1,N))\n",
    "    \n",
    "    rx_power1 = torch.mul(H1, p1)\n",
    "    rx_power1 = torch.sum(rx_power1,axis=-1)\n",
    "\n",
    "    rx_power2 = torch.mul(H2, p2)\n",
    "    rx_power2 = torch.sum(rx_power2,axis=-1)\n",
    "\n",
    "    rx_power3 = torch.mul(H1, p2)\n",
    "    rx_power3 = torch.sum(rx_power3,axis=-1)\n",
    "\n",
    "    rx_power4 = torch.mul(H2, p1)\n",
    "    rx_power4 = torch.sum(rx_power4,axis=-1)\n",
    "\n",
    "    rx_power = torch.mul(rx_power1 - rx_power2,rx_power1 - rx_power2) + torch.mul(rx_power3 + rx_power4,rx_power3 + rx_power4)\n",
    "    mask = torch.eye(K, device = device)\n",
    "    valid_rx_power = torch.sum(torch.mul(rx_power, mask), axis=1)\n",
    "    interference = torch.sum(torch.mul(rx_power, 1 - mask), axis=1) + 1\n",
    "    rate = torch.log2(1 + torch.div(valid_rx_power, interference))\n",
    "    sum_rate = torch.mean(torch.sum(rate, axis=1))\n",
    "    loss = torch.neg(sum_rate)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f81ffa-b843-41db-a177-de5510c28f8d",
   "metadata": {},
   "source": [
    "### Training Function\n",
    "\n",
    "The `train` function trains a neural network model using the specified optimizer and loss function.\n",
    "\n",
    "1. **Model Preparation**:\n",
    "   - Sets the model to training mode.\n",
    "\n",
    "2. **Loss Calculation Loop**:\n",
    "   - Iterates over the training data batches provided by `train_loader`.\n",
    "   - Transfers the data to the specified device (e.g., GPU).\n",
    "   - Resets the gradients of the optimizer.\n",
    "   - Passes the data through the model to obtain predictions.\n",
    "   - Calculates the loss using the `sr_loss` function, which computes the sum rate loss based on the received power and interference.\n",
    "   - Backpropagates the loss to compute gradients.\n",
    "   - Updates the total loss by adding the loss multiplied by the number of graphs (data instances) in the batch.\n",
    "   - Performs optimization by calling `optimizer.step()` to update the model parameters based on the computed gradients.\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "   - After processing all batches, computes the average loss per layout by dividing the total loss by the number of layouts.\n",
    "\n",
    "4. **Return**:\n",
    "   - Returns the average loss per layout as the output of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21ec0e7a-3ad5-4c6f-a483-68f893e2477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = sr_loss(data,out,train_K,Nt)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / train_layouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d23710-da21-4077-a4c8-f1169a143cc3",
   "metadata": {},
   "source": [
    "### Testing Function\n",
    "\n",
    "The `test` function evaluates the neural network model on the test dataset.\n",
    "1. **Model Evaluation**:\n",
    "   - Sets the model to evaluation mode.\n",
    "\n",
    "2. **Loss Calculation Loop**:\n",
    "   - Iterates over the test data batches provided by `test_loader`.\n",
    "   - Transfers the data to the specified device (e.g., GPU).\n",
    "   - Disables gradient computation using `torch.no_grad()` to speed up computation and save memory.\n",
    "   - Records the start time before model inference.\n",
    "   - Passes the data through the model to obtain predictions.\n",
    "   - Records the end time after model inference and prints the time taken.\n",
    "   - Calculates the loss using the `sr_loss` function, which computes the sum rate loss based on the received power and interference.\n",
    "   - Updates the total loss by adding the loss multiplied by the number of graphs (data instances) in the batch.\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "   - After processing all batches, computes the average loss per layout by dividing the total loss by the number of layouts.\n",
    "\n",
    "4. **Return**:\n",
    "   - Returns the average loss per layout as the output of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "085cb41e-5d59-4848-aaf6-8630df4ebe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            out = model(data)\n",
    "            end = time.time()\n",
    "            print('CGCNet time:', end-start)\n",
    "            loss = sr_loss(data,out,test_K,Nt)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            #power = out[:,:2*Nt]\n",
    "            #Y = power.numpy()\n",
    "            #power_check(Y)\n",
    "    \n",
    "    return total_loss / test_layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51defba5-455d-4deb-a962-da85bdd5bd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<20 layouts: 30_links_1000X1000_2_65_length>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<10 layouts: 30_links_1000X1000_2_65_length>>>>>>>>>>>>\n",
      "0.8329697004688061 44.13261591703872 0.008220171157523819 3.459562246262484\n",
      "0.7958741830422255 51.05758305063691 0.0015287609592706966 3.871541301572302\n",
      "WMMSE time: 2.5465402603149414\n",
      "WMMSE rate: 108.32834655515748\n",
      "Device: cpu\n",
      "Model: IGCNet\n",
      "Optimizer: Adam\n",
      "Scheduler: StepLR(step_size=20, gamma=0.9)\n",
      "Data Loading completed ....\n",
      "train_loader: Batch 1: Size = 20\n",
      "test_loader: Batch 1: Size = 10\n",
      "CGCNet time: 0.003065824508666992\n",
      "Epoch 001, Train Loss: -50.1422, Val Loss: -63.8789\n",
      "CGCNet time: 0.004076480865478516\n",
      "Epoch 002, Train Loss: -52.0666, Val Loss: -65.1230\n",
      "CGCNet time: 0.004507303237915039\n",
      "Epoch 003, Train Loss: -53.1808, Val Loss: -66.0039\n",
      "CGCNet time: 0.0036160945892333984\n",
      "Epoch 004, Train Loss: -54.1842, Val Loss: -66.6687\n",
      "CGCNet time: 0.004757881164550781\n",
      "Epoch 005, Train Loss: -55.2323, Val Loss: -67.2583\n",
      "CGCNet time: 0.006839275360107422\n",
      "Epoch 006, Train Loss: -56.3220, Val Loss: -67.7586\n",
      "CGCNet time: 0.003740072250366211\n",
      "Epoch 007, Train Loss: -57.3086, Val Loss: -68.3049\n",
      "CGCNet time: 0.01522517204284668\n",
      "Epoch 008, Train Loss: -58.1570, Val Loss: -68.7247\n",
      "CGCNet time: 0.010033845901489258\n",
      "Epoch 009, Train Loss: -58.9114, Val Loss: -69.0422\n",
      "CGCNet time: 0.004267692565917969\n",
      "Epoch 010, Train Loss: -59.6034, Val Loss: -69.3420\n",
      "CGCNet time: 0.0036573410034179688\n",
      "Epoch 011, Train Loss: -60.2493, Val Loss: -69.7495\n",
      "CGCNet time: 0.003111600875854492\n",
      "Epoch 012, Train Loss: -60.8328, Val Loss: -70.3453\n",
      "CGCNet time: 0.0029783248901367188\n",
      "Epoch 013, Train Loss: -61.4501, Val Loss: -70.8694\n",
      "CGCNet time: 0.003273487091064453\n",
      "Epoch 014, Train Loss: -61.9570, Val Loss: -71.2602\n",
      "CGCNet time: 0.0027649402618408203\n",
      "Epoch 015, Train Loss: -62.3497, Val Loss: -71.5130\n",
      "CGCNet time: 0.006038188934326172\n",
      "Epoch 016, Train Loss: -62.8676, Val Loss: -71.6552\n",
      "CGCNet time: 0.0029141902923583984\n",
      "Epoch 017, Train Loss: -63.4031, Val Loss: -71.8116\n",
      "CGCNet time: 0.0036411285400390625\n",
      "Epoch 018, Train Loss: -63.8346, Val Loss: -72.1066\n",
      "CGCNet time: 0.012974739074707031\n",
      "Epoch 019, Train Loss: -64.2780, Val Loss: -72.4709\n",
      "<<<<<<<<<<<<<50 layouts: 40_links_1000X1000_2_65_length>>>>>>>>>>>>\n",
      "test size (50, 40, 40, 2) 1154\n",
      "WMMSE time: 16.247117042541504\n",
      "WMMSE rate: 126.97833209445824\n",
      "0.8329697004688061 44.13261591703872 0.008220171157523819 3.459562246262484\n",
      "0.7958741830422255 51.05758305063691 0.0015287609592706966 3.871541301572302\n",
      "CGCNet time: 0.03354477882385254\n",
      "CGCNet rate: -79.91409301757812\n",
      "<<<<<<<<<<<<<50 layouts: 80_links_1000X1000_2_65_length>>>>>>>>>>>>\n",
      "test size (50, 80, 80, 2) 1632\n",
      "WMMSE time: 32.41821885108948\n",
      "WMMSE rate: 229.26104556646442\n",
      "0.8329697004688061 44.13261591703872 0.008220171157523819 3.459562246262484\n",
      "0.7958741830422255 51.05758305063691 0.0015287609592706966 3.871541301572302\n",
      "CGCNet time: 0.13134407997131348\n",
      "CGCNet rate: -144.21917724609375\n",
      "<<<<<<<<<<<<<50 layouts: 160_links_1000X1000_2_65_length>>>>>>>>>>>>\n",
      "test size (50, 160, 160, 2) 2309\n",
      "WMMSE time: 70.58055830001831\n",
      "WMMSE rate: 422.7550292945985\n",
      "0.8329697004688061 44.13261591703872 0.008220171157523819 3.459562246262484\n",
      "0.7958741830422255 51.05758305063691 0.0015287609592706966 3.871541301572302\n",
      "CGCNet time: 0.2841756343841553\n",
      "CGCNet rate: -261.788818359375\n",
      "EoD\n"
     ]
    }
   ],
   "source": [
    "Nt  = 2\n",
    "var = 1\n",
    "\n",
    "test_K  = 30\n",
    "train_K = 30\n",
    "train_layouts = 20\n",
    "test_layouts  = 10\n",
    "\n",
    "train_config = init_parameters(train_K, Nt)\n",
    "test_config  = init_parameters(test_K, Nt)\n",
    "\n",
    "(train_dists, train_csis) = sample_generate(train_config, train_layouts)\n",
    "(test_dists, test_csis)   = sample_generate(test_config, test_layouts)\n",
    "\n",
    "(train_csi_real, train_csi_imag) = np.real(train_csis), np.imag(train_csis)\n",
    "(test_csi_real, test_csi_imag)   = np.real(test_csis), np.imag(test_csis)\n",
    "\n",
    "\n",
    "(norm_train_real, norm_test_real) = normalize_data(train_csi_real, test_csi_real, train_config)\n",
    "(norm_train_imag, norm_test_imag) = normalize_data(train_csi_imag, test_csi_imag, train_config)\n",
    "\n",
    "start = time.time()\n",
    "Y = batch_wmmse(test_csis.transpose(0,2,1,3),var)\n",
    "end = time.time()\n",
    "print('WMMSE time:',end-start)\n",
    "sr = IC_sum_rate( test_csis,Y,var)\n",
    "print('WMMSE rate:',sr)\n",
    "\n",
    "train_data_list = proc_data(train_csis, train_dists, norm_train_real, norm_train_imag, train_K)\n",
    "test_data_list  = proc_data(test_csis, test_dists, norm_test_real, norm_test_imag,  test_K)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Initialize device\n",
    "model  = IGCNet().to(device) # Initialize model\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Initialize optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9) # Initialize scheduler\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: {model.__class__.__name__}\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Scheduler: StepLR(step_size=20, gamma=0.9)\")\n",
    "\n",
    "train_loader = DataLoader(train_data_list, batch_size=64, shuffle=True,num_workers=0)\n",
    "test_loader  = DataLoader(test_data_list, batch_size=test_layouts, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Data Loading completed ....\")\n",
    "# Iterate over both train_loader and test_loader\n",
    "for loader_name, loader in [(\"train_loader\", train_loader), (\"test_loader\", test_loader)]:\n",
    "    # Iterate over the DataLoader and print the size of each batch\n",
    "    for i, batch in enumerate(loader, 1):\n",
    "        batch_size = batch.num_graphs  # Get the number of graphs in the batch\n",
    "        print(f\"{loader_name}: Batch {i}: Size = {batch_size}\")\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    loss1 = train()\n",
    "    \n",
    "    loss2 = test()\n",
    "    print('Epoch {:03d}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(\n",
    "        epoch, loss1, loss2))\n",
    "    scheduler.step()\n",
    "    \n",
    "density   = test_config.field_length**2/test_K\n",
    "gen_tests = [40, 80, 160]\n",
    "for test_K in gen_tests:\n",
    "    test_layouts = 50\n",
    "    test_config = init_parameters(test_K, Nt)\n",
    "    field_length = int(np.sqrt(density*test_K))\n",
    "    test_config.field_length = field_length\n",
    "    test_dists, test_csis    = sample_generate(test_config, test_layouts)\n",
    "    print('test size', test_csis.shape,field_length)\n",
    "\n",
    "    start = time.time()\n",
    "    start = time.time()\n",
    "    Y = batch_wmmse(test_csis.transpose(0,2,1,3),var)\n",
    "    end = time.time()\n",
    "    print('WMMSE time:',end-start)\n",
    "    sr = IC_sum_rate( test_csis,Y,var)\n",
    "    print('WMMSE rate:',sr)\n",
    "\n",
    "    test_csi_real, test_csi_imag = np.real(test_csis), np.imag(test_csis)\n",
    "    _, norm_test_real = normalize_data(train_csi_real,test_csi_real, train_config)\n",
    "    _, norm_test_imag = normalize_data(train_csi_imag,test_csi_imag, test_config)\n",
    "\n",
    "    test_data_list = proc_data(test_csis, test_dists, norm_test_real, norm_test_imag,  test_K)\n",
    "    test_loader = DataLoader(test_data_list, batch_size=test_layouts, shuffle=False, num_workers=1)\n",
    "    loss2 = test()\n",
    "    print('CGCNet rate:',loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b601d-933b-4996-9b22-66d0625356ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f31c58-ca55-4424-9fbc-03a81e7e8714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
